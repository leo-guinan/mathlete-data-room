{
  "tweet": {
    "edit_info": {
      "initial": {
        "editTweetIds": [
          "1604172179389616129"
        ],
        "editableUntil": "2022-12-17T18:20:26.000Z",
        "editsRemaining": "5",
        "isEditEligible": false
      }
    },
    "retweeted": false,
    "source": "<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>",
    "entities": {
      "hashtags": [],
      "symbols": [],
      "user_mentions": [
        {
          "name": "LangChain",
          "screen_name": "LangChainAI",
          "indices": [
            "16",
            "28"
          ],
          "id_str": "1589007443853340672",
          "id": "1589007443853340672"
        }
      ],
      "urls": []
    },
    "display_text_range": [
      "0",
      "174"
    ],
    "favorite_count": "1",
    "in_reply_to_status_id_str": "1603820722840752129",
    "id_str": "1604172179389616129",
    "in_reply_to_user_id": "1467701028170739714",
    "truncated": false,
    "retweet_count": "0",
    "id": "1604172179389616129",
    "in_reply_to_status_id": "1603820722840752129",
    "created_at": "Sat Dec 17 17:50:26 +0000 2022",
    "favorited": false,
    "full_text": "@ArtificialNate @LangChainAI Mostly to process longer inputs, specifically podcast transcripts. Since the APIs have a size limit, I need to break things up before processing.",
    "lang": "en",
    "in_reply_to_screen_name": "nate__pratt",
    "in_reply_to_user_id_str": "1467701028170739714"
  }
}