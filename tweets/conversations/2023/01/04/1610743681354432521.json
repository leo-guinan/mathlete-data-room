{
  "tweet": {
    "edit_info": {
      "initial": {
        "editTweetIds": [
          "1610743681354432521"
        ],
        "editableUntil": "2023-01-04T21:33:14.000Z",
        "editsRemaining": "5",
        "isEditEligible": false
      }
    },
    "retweeted": false,
    "source": "<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>",
    "entities": {
      "hashtags": [],
      "symbols": [],
      "user_mentions": [
        {
          "name": "Caspian Almerud",
          "screen_name": "AlmerudCaspian",
          "indices": [
            "0",
            "15"
          ],
          "id_str": "2999734180",
          "id": "2999734180"
        }
      ],
      "urls": []
    },
    "display_text_range": [
      "0",
      "284"
    ],
    "favorite_count": "2",
    "in_reply_to_status_id_str": "1610743362730196992",
    "id_str": "1610743681354432521",
    "in_reply_to_user_id": "1325102346792218629",
    "truncated": false,
    "retweet_count": "0",
    "id": "1610743681354432521",
    "in_reply_to_status_id": "1610743362730196992",
    "created_at": "Wed Jan 04 21:03:14 +0000 2023",
    "favorited": false,
    "full_text": "@AlmerudCaspian Instead, I'm planning on using the current large language models to cheaply create my own distribution algorithm instead. It doesn't need nearly as much data because it doesn't have nearly enough volume.\n\nAnd that's totally fine as an individual.\n\nHere's the approach:",
    "lang": "en",
    "in_reply_to_screen_name": "leo_guinan",
    "in_reply_to_user_id_str": "1325102346792218629"
  }
}