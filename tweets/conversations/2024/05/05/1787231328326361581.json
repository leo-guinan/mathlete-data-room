{
  "tweet": {
    "edit_info": {
      "initial": {
        "editTweetIds": [
          "1787231328326361581"
        ],
        "editableUntil": "2024-05-05T22:22:08.000Z",
        "editsRemaining": "5",
        "isEditEligible": false
      }
    },
    "retweeted": false,
    "source": "<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>",
    "entities": {
      "hashtags": [],
      "symbols": [],
      "user_mentions": [],
      "urls": []
    },
    "display_text_range": [
      "0",
      "182"
    ],
    "favorite_count": "0",
    "in_reply_to_status_id_str": "1787231326724116823",
    "id_str": "1787231328326361581",
    "in_reply_to_user_id": "1325102346792218629",
    "truncated": false,
    "retweet_count": "0",
    "id": "1787231328326361581",
    "in_reply_to_status_id": "1787231326724116823",
    "created_at": "Sun May 05 21:22:08 +0000 2024",
    "favorited": false,
    "full_text": "By doing breaking down a question into subquestions a couple of times, I've found that you can effectively use this as a \"map\" of latent space for the LLM to construct its answer in.",
    "lang": "en",
    "in_reply_to_screen_name": "leo_guinan",
    "in_reply_to_user_id_str": "1325102346792218629"
  }
}