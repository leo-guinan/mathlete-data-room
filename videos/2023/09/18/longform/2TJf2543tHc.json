{
  "id": "2TJf2543tHc",
  "channel_id": "UC_Mn3zTIipnTQ8Zy_QtXZew",
  "title": "Engineer Reacts to Elon's Last AI Warning! ðŸ˜±ðŸ¤–",
  "description": "Today it's time to react to Elon Musk's 'Final Warning' about AI. Looking at his arguments, understanding of the subject and how AI really works.\n\nLets jump in!\n\nCheck out our new tech moves with: https://internetfactory.substack.com/\n\nJoin the conversation and share your thoughts in the comments below.\n_____________ \n\nDon't forget to like, comment & subscribe for so you don't miss out!\n\n#elonmusk #technology #reacts #ai",
  "published_at": "2023-09-18T17:00:08+00:00",
  "view_count": 256,
  "like_count": 8,
  "comment_count": 0,
  "duration": "PT32M1S",
  "tags": [
    "elon",
    "new",
    "ai",
    "reacts",
    "reaction",
    "technology",
    "w2s",
    "warning"
  ],
  "transcript": "Kind: captions Language: en you<00:00:00.420><c> see</c><00:00:00.539><c> me</c><00:00:00.719><c> on</c><00:00:00.840><c> here</c><00:00:00.960><c> before</c><00:00:01.079><c> chances</c><00:00:01.860><c> are</c> you see me on here before chances are you see me on here before chances are you've<00:00:03.540><c> seen</c><00:00:03.959><c> some</c><00:00:04.140><c> of</c><00:00:04.259><c> the</c><00:00:04.319><c> reactions</c><00:00:04.680><c> I've</c> you've seen some of the reactions I've you've seen some of the reactions I've had<00:00:05.040><c> to</c><00:00:05.460><c> Elon</c><00:00:06.299><c> Musk</c><00:00:06.660><c> and</c><00:00:07.500><c> some</c><00:00:07.799><c> of</c><00:00:07.859><c> the</c><00:00:07.980><c> things</c> had to Elon Musk and some of the things had to Elon Musk and some of the things he's<00:00:08.340><c> done</c> he's done he's done but<00:00:10.740><c> it's</c><00:00:10.860><c> usually</c><00:00:11.099><c> just</c><00:00:11.280><c> in</c><00:00:11.460><c> passing</c><00:00:12.019><c> in</c><00:00:13.019><c> this</c> but it's usually just in passing in this but it's usually just in passing in this case<00:00:13.320><c> I</c><00:00:13.799><c> wanted</c><00:00:13.920><c> to</c><00:00:14.160><c> actually</c><00:00:14.280><c> do</c><00:00:14.519><c> a</c><00:00:14.639><c> live</c> case I wanted to actually do a live case I wanted to actually do a live reaction reaction reaction [Music] [Music] [Music] [Applause] welcome<00:00:24.960><c> back</c><00:00:25.140><c> to</c><00:00:25.260><c> listen</c><00:00:25.380><c> to</c><00:00:25.619><c> Leo</c><00:00:26.340><c> in</c><00:00:26.820><c> today's</c> welcome back to listen to Leo in today's welcome back to listen to Leo in today's video video video we're<00:00:28.439><c> gonna</c><00:00:28.560><c> see</c><00:00:28.920><c> what</c><00:00:29.460><c> Elon</c><00:00:30.119><c> has</c><00:00:30.359><c> to</c><00:00:30.539><c> say</c><00:00:30.720><c> is</c> we're gonna see what Elon has to say is we're gonna see what Elon has to say is his<00:00:31.740><c> last</c><00:00:32.160><c> warning</c> danger<00:00:36.180><c> of</c><00:00:36.420><c> AI</c><00:00:36.719><c> is</c><00:00:36.899><c> much</c><00:00:37.260><c> greater</c><00:00:37.620><c> than</c><00:00:37.920><c> the</c> danger of AI is much greater than the danger of AI is much greater than the the<00:00:38.280><c> danger</c><00:00:38.760><c> of</c><00:00:39.000><c> nuclear</c><00:00:39.300><c> warheads</c><00:00:39.719><c> by</c><00:00:39.960><c> a</c><00:00:40.200><c> lot</c> okay okay okay so so so if<00:00:48.420><c> we</c><00:00:48.539><c> start</c><00:00:48.719><c> with</c><00:00:48.840><c> that</c><00:00:48.960><c> first</c><00:00:49.079><c> statement</c> if we start with that first statement if we start with that first statement the<00:00:50.520><c> danger</c><00:00:50.879><c> of</c><00:00:51.059><c> AI</c> the danger of AI the danger of AI it's<00:00:52.920><c> much</c><00:00:53.219><c> greater</c><00:00:53.460><c> than</c><00:00:53.520><c> that</c><00:00:53.640><c> of</c><00:00:53.820><c> nuclear</c> it's much greater than that of nuclear it's much greater than that of nuclear warheads I<00:00:57.300><c> don't</c><00:00:57.420><c> think</c><00:00:57.600><c> that's</c><00:00:58.160><c> exactly</c><00:00:59.160><c> accurate</c> I don't think that's exactly accurate I don't think that's exactly accurate right<00:01:00.239><c> now</c> right now right now I<00:01:02.399><c> think</c><00:01:02.579><c> it</c><00:01:03.359><c> could</c><00:01:03.480><c> be</c><00:01:03.660><c> true</c><00:01:03.979><c> in</c><00:01:04.979><c> the</c><00:01:05.100><c> future</c> I think it could be true in the future I think it could be true in the future but<00:01:06.960><c> at</c><00:01:07.320><c> the</c><00:01:07.439><c> current</c><00:01:07.680><c> state</c><00:01:07.920><c> of</c><00:01:08.100><c> Technology</c> but at the current state of Technology but at the current state of Technology I<00:01:10.020><c> will</c><00:01:10.260><c> take</c><00:01:10.439><c> AI</c><00:01:10.920><c> over</c><00:01:11.220><c> Warheads</c><00:01:11.880><c> the</c><00:01:12.360><c> problem</c> I will take AI over Warheads the problem I will take AI over Warheads the problem is<00:01:12.780><c> AI</c><00:01:13.619><c> does</c><00:01:14.100><c> have</c><00:01:14.939><c> much</c><00:01:15.180><c> better</c><00:01:15.420><c> potential</c><00:01:15.900><c> to</c> is AI does have much better potential to is AI does have much better potential to evolve<00:01:17.340><c> much</c><00:01:18.060><c> more</c><00:01:18.299><c> rapidly</c><00:01:18.720><c> than</c><00:01:19.200><c> do</c><00:01:19.439><c> let's</c> evolve much more rapidly than do let's evolve much more rapidly than do let's say<00:01:19.799><c> nuclear</c><00:01:20.220><c> warheads</c><00:01:20.700><c> so</c><00:01:21.600><c> we'll</c><00:01:22.320><c> see</c><00:01:22.500><c> when</c> say nuclear warheads so we'll see when say nuclear warheads so we'll see when that<00:01:22.799><c> comes</c><00:01:23.040><c> but</c><00:01:23.340><c> anyway</c> that comes but anyway that comes but anyway let's<00:01:24.960><c> see</c><00:01:25.200><c> if</c><00:01:25.200><c> we</c><00:01:25.320><c> can</c><00:01:25.380><c> get</c><00:01:25.500><c> past</c><00:01:25.680><c> the</c><00:01:25.979><c> first</c> let's see if we can get past the first let's see if we can get past the first sentence<00:01:27.000><c> shall</c><00:01:27.240><c> we</c> mark<00:01:30.299><c> my</c><00:01:30.600><c> words</c> mark my words mark my words AI<00:01:32.040><c> is</c><00:01:32.220><c> far</c><00:01:32.700><c> more</c><00:01:32.880><c> dangerous</c><00:01:33.299><c> than</c><00:01:33.960><c> nukes</c> AI is far more dangerous than nukes AI is far more dangerous than nukes again<00:01:36.540><c> with</c><00:01:36.840><c> the</c><00:01:36.960><c> far</c><00:01:37.079><c> more</c><00:01:37.259><c> dangerous</c><00:01:37.560><c> I</c><00:01:37.680><c> try</c> again with the far more dangerous I try again with the far more dangerous I try to<00:01:37.920><c> convince</c><00:01:38.220><c> people</c><00:01:38.340><c> to</c><00:01:38.520><c> slow</c><00:01:38.700><c> down</c> to convince people to slow down to convince people to slow down slow<00:01:40.259><c> down</c><00:01:40.439><c> AI</c> slow down AI slow down AI to<00:01:42.119><c> regulate</c><00:01:42.479><c> AI</c> this<00:01:45.600><c> was</c><00:01:45.780><c> futile</c> this was futile this was futile I<00:01:47.880><c> tried</c><00:01:48.060><c> for</c><00:01:48.299><c> years</c> resistance<00:01:51.659><c> is</c><00:01:51.780><c> futile</c><00:01:52.259><c> the</c><00:01:52.619><c> base</c><00:01:52.860><c> issue</c><00:01:53.220><c> I</c> resistance is futile the base issue I resistance is futile the base issue I see<00:01:53.520><c> with</c><00:01:53.820><c> so-called</c><00:01:54.540><c> AI</c><00:01:54.899><c> experts</c><00:01:55.320><c> is</c><00:01:55.439><c> that</c> see with so-called AI experts is that see with so-called AI experts is that they<00:01:55.920><c> they</c><00:01:56.159><c> think</c><00:01:56.399><c> they</c><00:01:56.700><c> know</c><00:01:56.820><c> more</c><00:01:57.060><c> than</c><00:01:57.180><c> they</c> they they think they know more than they they they think they know more than they do do do um um um and<00:01:59.579><c> I</c><00:01:59.820><c> think</c><00:01:59.939><c> they're</c><00:02:00.180><c> smarter</c><00:02:00.540><c> than</c><00:02:00.659><c> they</c> and I think they're smarter than they and I think they're smarter than they actually<00:02:01.020><c> are</c> actually are actually are this<00:02:03.180><c> is</c><00:02:03.299><c> this</c><00:02:03.479><c> tends</c><00:02:03.780><c> to</c><00:02:03.899><c> play</c><00:02:04.079><c> plague</c><00:02:04.740><c> smart</c> this is this tends to play plague smart this is this tends to play plague smart people<00:02:05.219><c> they</c><00:02:05.700><c> Define</c><00:02:05.939><c> themselves</c><00:02:06.479><c> by</c><00:02:06.659><c> their</c> people they Define themselves by their people they Define themselves by their intelligence<00:02:07.259><c> and</c><00:02:07.799><c> they</c><00:02:08.160><c> they</c><00:02:08.700><c> don't</c><00:02:08.940><c> like</c> intelligence and they they don't like intelligence and they they don't like the<00:02:09.539><c> idea</c><00:02:09.780><c> that</c><00:02:10.140><c> a</c><00:02:10.440><c> machine</c><00:02:10.619><c> can</c><00:02:10.860><c> be</c><00:02:11.039><c> way</c> the idea that a machine can be way the idea that a machine can be way smarter<00:02:11.580><c> than</c><00:02:11.640><c> them</c><00:02:11.819><c> so</c><00:02:12.000><c> they</c><00:02:12.180><c> discount</c><00:02:12.300><c> the</c> smarter than them so they discount the smarter than them so they discount the idea<00:02:12.900><c> which</c><00:02:13.319><c> is</c> idea which is idea which is fundamentally<00:02:15.180><c> forward</c><00:02:15.420><c> that's</c><00:02:15.720><c> The</c><00:02:15.900><c> Wishful</c> fundamentally forward that's The Wishful fundamentally forward that's The Wishful Thinking<00:02:16.560><c> situation</c><00:02:17.520><c> I'm</c><00:02:18.180><c> really</c><00:02:18.420><c> quite</c> Thinking situation I'm really quite Thinking situation I'm really quite close<00:02:18.959><c> to</c><00:02:19.280><c> very</c><00:02:20.280><c> close</c><00:02:20.400><c> to</c><00:02:20.819><c> The</c><00:02:21.180><c> Cutting</c><00:02:21.420><c> Edge</c> close to very close to The Cutting Edge close to very close to The Cutting Edge in<00:02:21.900><c> Ai</c><00:02:22.400><c> and</c><00:02:23.400><c> it</c><00:02:23.879><c> scares</c><00:02:24.239><c> the</c><00:02:24.360><c> hell</c><00:02:24.480><c> out</c><00:02:24.599><c> of</c><00:02:24.720><c> me</c> in Ai and it scares the hell out of me in Ai and it scares the hell out of me it's<00:02:25.560><c> capable</c><00:02:25.920><c> of</c><00:02:26.099><c> vastly</c><00:02:26.520><c> more</c><00:02:26.700><c> than</c><00:02:26.879><c> almost</c> it's capable of vastly more than almost it's capable of vastly more than almost anyone<00:02:27.540><c> knows</c> anyone knows anyone knows yeah yeah yeah so<00:02:31.920><c> pause</c><00:02:32.220><c> it</c><00:02:32.459><c> on</c><00:02:32.640><c> this</c><00:02:33.300><c> piece</c><00:02:34.080><c> stating</c><00:02:34.620><c> the</c> so pause it on this piece stating the so pause it on this piece stating the rate<00:02:34.980><c> of</c><00:02:35.160><c> improvement</c><00:02:35.520><c> is</c><00:02:35.879><c> exponential</c> rate of improvement is exponential rate of improvement is exponential um this<00:02:37.860><c> is</c><00:02:37.920><c> true</c><00:02:38.160><c> this</c><00:02:38.879><c> is</c><00:02:39.000><c> something</c><00:02:39.180><c> that</c><00:02:40.140><c> I</c> um this is true this is something that I um this is true this is something that I think<00:02:40.739><c> most</c><00:02:41.040><c> people</c><00:02:41.280><c> don't</c><00:02:41.640><c> understand</c><00:02:42.019><c> I</c><00:02:43.019><c> see</c> think most people don't understand I see think most people don't understand I see a<00:02:43.379><c> lot</c><00:02:43.500><c> of</c><00:02:43.560><c> people</c><00:02:43.680><c> talking</c><00:02:43.980><c> about</c><00:02:44.280><c> the</c><00:02:44.580><c> fact</c> a lot of people talking about the fact a lot of people talking about the fact they're<00:02:45.420><c> like</c><00:02:45.599><c> oh</c><00:02:46.140><c> I</c><00:02:46.319><c> tried</c><00:02:46.500><c> chat</c><00:02:46.800><c> GPT</c><00:02:47.220><c> and</c><00:02:47.760><c> it</c> they're like oh I tried chat GPT and it they're like oh I tried chat GPT and it said<00:02:48.420><c> this</c><00:02:48.660><c> and</c><00:02:48.900><c> isn't</c><00:02:49.200><c> that</c><00:02:49.319><c> ridiculous</c> said this and isn't that ridiculous said this and isn't that ridiculous not<00:02:51.720><c> realizing</c><00:02:52.260><c> that</c><00:02:52.620><c> the</c><00:02:52.800><c> fact</c><00:02:52.920><c> of</c><00:02:53.099><c> the</c> not realizing that the fact of the not realizing that the fact of the matter<00:02:53.340><c> is</c><00:02:53.459><c> these</c><00:02:53.819><c> models</c><00:02:54.120><c> are</c><00:02:54.360><c> actually</c> matter is these models are actually matter is these models are actually improving<00:02:55.080><c> quite</c><00:02:55.379><c> rapidly</c> improving quite rapidly improving quite rapidly um um um and<00:02:58.260><c> I</c><00:02:58.379><c> would</c><00:02:58.500><c> also</c><00:02:58.739><c> agree</c><00:02:59.040><c> that</c><00:02:59.459><c> most</c><00:02:59.640><c> smart</c> and I would also agree that most smart and I would also agree that most smart people<00:03:00.060><c> think</c><00:03:00.360><c> that</c><00:03:00.540><c> they're</c><00:03:00.720><c> smarter</c><00:03:01.019><c> than</c> people think that they're smarter than people think that they're smarter than they<00:03:01.379><c> are</c> they are they are um I<00:03:06.959><c> still</c><00:03:07.140><c> think</c><00:03:07.560><c> that</c> AI<00:03:11.640><c> has</c><00:03:12.060><c> a</c><00:03:12.360><c> lot</c><00:03:12.480><c> more</c><00:03:12.659><c> potential</c><00:03:13.200><c> to</c><00:03:13.800><c> be</c> AI has a lot more potential to be AI has a lot more potential to be way<00:03:16.620><c> better</c><00:03:16.920><c> than</c><00:03:17.400><c> humans</c> way better than humans way better than humans um um um in<00:03:19.980><c> a</c><00:03:20.040><c> lot</c><00:03:20.159><c> of</c><00:03:20.280><c> areas</c><00:03:20.760><c> and</c><00:03:21.239><c> this</c><00:03:21.420><c> is</c><00:03:21.540><c> something</c> in a lot of areas and this is something in a lot of areas and this is something that<00:03:22.019><c> I</c><00:03:22.200><c> think</c> that I think that I think people<00:03:24.180><c> are</c><00:03:24.659><c> going</c><00:03:24.900><c> to</c><00:03:25.140><c> have</c><00:03:25.379><c> a</c><00:03:25.500><c> very</c><00:03:25.620><c> hard</c> people are going to have a very hard people are going to have a very hard time<00:03:25.980><c> with</c><00:03:26.280><c> I</c><00:03:26.760><c> think</c> time with I think time with I think AI<00:03:29.220><c> in</c><00:03:29.459><c> and</c><00:03:29.640><c> of</c><00:03:29.760><c> itself</c><00:03:30.120><c> is</c><00:03:30.659><c> not</c><00:03:30.900><c> necessarily</c> AI in and of itself is not necessarily AI in and of itself is not necessarily the<00:03:31.560><c> danger</c> the danger the danger I<00:03:33.120><c> think</c> I think I think people's<00:03:34.680><c> response</c><00:03:35.459><c> to</c><00:03:35.760><c> what</c><00:03:36.000><c> AI</c><00:03:36.360><c> offers</c> people's response to what AI offers people's response to what AI offers is<00:03:38.640><c> a</c><00:03:38.940><c> much</c><00:03:39.120><c> higher</c><00:03:39.720><c> threat</c><00:03:40.200><c> than</c><00:03:40.980><c> AI</c><00:03:41.580><c> itself</c><00:03:42.180><c> I</c> is a much higher threat than AI itself I is a much higher threat than AI itself I don't<00:03:42.659><c> think</c><00:03:42.840><c> AI</c><00:03:43.319><c> is</c><00:03:43.620><c> going</c><00:03:43.860><c> to</c><00:03:44.099><c> decide</c><00:03:44.580><c> to</c><00:03:45.360><c> do</c> don't think AI is going to decide to do don't think AI is going to decide to do something<00:03:45.720><c> catastrophic</c> something catastrophic something catastrophic I<00:03:47.940><c> think</c><00:03:48.000><c> that</c><00:03:48.120><c> unless</c><00:03:48.420><c> we</c><00:03:48.599><c> change</c><00:03:48.780><c> social</c> I think that unless we change social I think that unless we change social structures<00:03:49.620><c> to</c><00:03:50.040><c> account</c><00:03:50.220><c> for</c><00:03:50.879><c> differences</c><00:03:51.540><c> in</c> structures to account for differences in structures to account for differences in employment employment employment um I<00:03:54.720><c> think</c><00:03:55.019><c> we're</c><00:03:55.319><c> going</c><00:03:55.500><c> to</c><00:03:55.680><c> see</c> um I think we're going to see um I think we're going to see huge<00:03:57.599><c> gaps</c><00:03:58.379><c> even</c><00:03:58.680><c> more</c><00:03:59.040><c> so</c><00:03:59.159><c> than</c><00:03:59.340><c> now</c><00:03:59.580><c> in</c><00:03:59.940><c> terms</c> huge gaps even more so than now in terms huge gaps even more so than now in terms of<00:04:00.720><c> income</c><00:04:01.319><c> because</c><00:04:01.620><c> people</c><00:04:01.920><c> who</c><00:04:02.220><c> know</c><00:04:02.340><c> how</c><00:04:02.580><c> to</c> of income because people who know how to of income because people who know how to leverage<00:04:02.879><c> AI</c> leverage AI leverage AI can<00:04:04.980><c> achieve</c><00:04:05.760><c> much</c><00:04:06.180><c> higher</c> can achieve much higher can achieve much higher value<00:04:08.580><c> leverage</c><00:04:09.120><c> than</c><00:04:09.780><c> people</c><00:04:10.080><c> who</c><00:04:10.379><c> are</c><00:04:10.500><c> not</c> value leverage than people who are not value leverage than people who are not using<00:04:10.920><c> Ai</c><00:04:11.420><c> and</c><00:04:12.420><c> so</c><00:04:12.780><c> we're</c><00:04:13.080><c> going</c><00:04:13.319><c> to</c><00:04:13.620><c> see</c><00:04:14.040><c> the</c> using Ai and so we're going to see the using Ai and so we're going to see the people<00:04:14.939><c> who</c><00:04:15.180><c> understand</c><00:04:15.420><c> it</c> people who understand it people who understand it probably<00:04:17.100><c> get</c> probably get probably get stupid<00:04:18.840><c> rich</c><00:04:19.260><c> off</c><00:04:19.919><c> of</c><00:04:20.100><c> a</c><00:04:20.519><c> lot</c><00:04:20.639><c> of</c><00:04:20.760><c> this</c><00:04:20.880><c> it's</c> stupid rich off of a lot of this it's stupid rich off of a lot of this it's similar<00:04:21.540><c> to</c><00:04:21.660><c> the.com</c><00:04:22.139><c> Boom</c><00:04:22.759><c> there's</c><00:04:23.759><c> a</c><00:04:23.940><c> ton</c><00:04:24.060><c> of</c> similar to the.com Boom there's a ton of similar to the.com Boom there's a ton of money<00:04:24.300><c> flowing</c><00:04:24.660><c> into</c><00:04:24.780><c> the</c><00:04:24.960><c> space</c><00:04:25.080><c> right</c><00:04:25.500><c> now</c> money flowing into the space right now money flowing into the space right now people<00:04:26.759><c> who</c><00:04:27.060><c> know</c><00:04:27.180><c> how</c><00:04:27.360><c> to</c><00:04:27.419><c> build</c><00:04:27.540><c> who</c><00:04:27.780><c> know</c> people who know how to build who know people who know how to build who know how<00:04:28.080><c> to</c><00:04:28.139><c> leverage</c><00:04:28.380><c> are</c><00:04:28.860><c> going</c><00:04:29.160><c> to</c><00:04:29.460><c> set</c> how to leverage are going to set how to leverage are going to set themselves<00:04:30.300><c> up</c><00:04:30.600><c> for</c><00:04:30.960><c> a</c><00:04:31.259><c> lot</c><00:04:31.380><c> of</c><00:04:31.560><c> success</c><00:04:32.180><c> very</c> themselves up for a lot of success very themselves up for a lot of success very rapidly<00:04:33.919><c> and</c><00:04:34.919><c> because</c><00:04:35.340><c> of</c><00:04:35.580><c> that</c><00:04:35.759><c> difference</c> rapidly and because of that difference rapidly and because of that difference in<00:04:36.840><c> value</c><00:04:37.199><c> creation</c><00:04:37.800><c> I</c><00:04:38.759><c> think</c><00:04:38.880><c> that</c><00:04:39.660><c> stands</c><00:04:40.080><c> a</c> in value creation I think that stands a in value creation I think that stands a chance<00:04:40.440><c> to</c><00:04:41.280><c> disrupt</c><00:04:42.060><c> a</c><00:04:42.180><c> lot</c><00:04:42.240><c> of</c><00:04:42.360><c> the</c><00:04:42.419><c> world</c> chance to disrupt a lot of the world chance to disrupt a lot of the world economy economy economy which<00:04:44.699><c> is</c><00:04:44.820><c> going</c><00:04:45.000><c> to</c><00:04:45.120><c> cause</c><00:04:45.240><c> unrest</c><00:04:45.660><c> and</c> which is going to cause unrest and which is going to cause unrest and therefore therefore therefore I<00:04:48.360><c> I</c><00:04:48.360><c> see</c><00:04:49.139><c> people</c><00:04:49.680><c> being</c><00:04:50.100><c> at</c><00:04:50.340><c> the</c><00:04:50.460><c> root</c><00:04:50.699><c> of</c><00:04:50.880><c> the</c> I I see people being at the root of the I I see people being at the root of the problem<00:04:51.300><c> more</c><00:04:52.020><c> so</c><00:04:52.199><c> uh</c><00:04:53.100><c> than</c><00:04:53.340><c> I</c><00:04:53.520><c> am</c><00:04:53.639><c> but</c><00:04:53.820><c> anyway</c> problem more so uh than I am but anyway problem more so uh than I am but anyway let's<00:04:54.240><c> continue</c> let's continue let's continue a<00:04:56.520><c> bit</c><00:04:56.699><c> of</c><00:04:56.940><c> improvement</c><00:04:57.300><c> is</c><00:04:57.840><c> exponential</c> it<00:05:01.500><c> feels</c><00:05:01.860><c> like</c><00:05:02.040><c> we</c><00:05:02.280><c> are</c><00:05:02.460><c> the</c><00:05:02.940><c> biological</c> it feels like we are the biological it feels like we are the biological Bootloader<00:05:04.080><c> for</c><00:05:04.560><c> AI</c><00:05:05.040><c> effectively</c><00:05:06.000><c> we</c><00:05:06.479><c> are</c> Bootloader for AI effectively we are Bootloader for AI effectively we are building<00:05:07.080><c> it</c> building it building it and<00:05:09.660><c> then</c><00:05:09.780><c> we're</c><00:05:10.139><c> building</c> and then we're building and then we're building progressively<00:05:12.180><c> greater</c><00:05:13.080><c> intelligence</c> progressively greater intelligence progressively greater intelligence and<00:05:15.900><c> the</c><00:05:16.139><c> percentage</c><00:05:16.500><c> of</c><00:05:16.800><c> intelligence</c><00:05:17.280><c> that</c> and the percentage of intelligence that and the percentage of intelligence that is<00:05:17.759><c> not</c><00:05:18.000><c> human</c><00:05:18.240><c> is</c><00:05:19.199><c> increasing</c> is not human is increasing is not human is increasing and<00:05:20.759><c> eventually</c> and eventually and eventually we<00:05:22.320><c> will</c><00:05:22.500><c> represent</c><00:05:22.860><c> a</c><00:05:23.100><c> very</c><00:05:23.340><c> small</c> we will represent a very small we will represent a very small percentage<00:05:24.120><c> of</c><00:05:24.300><c> intelligence</c> percentage of intelligence percentage of intelligence it's<00:05:26.880><c> going</c><00:05:27.000><c> to</c><00:05:27.000><c> come</c><00:05:27.180><c> faster</c><00:05:27.419><c> than</c><00:05:27.660><c> anyone</c> it's going to come faster than anyone it's going to come faster than anyone appreciates<00:05:28.860><c> I</c><00:05:29.100><c> think</c><00:05:29.220><c> it's</c> appreciates I think it's appreciates I think it's with<00:05:30.419><c> each</c><00:05:30.900><c> passing</c><00:05:31.259><c> year</c><00:05:31.440><c> the</c> with each passing year the with each passing year the sophistication<00:05:32.340><c> of</c><00:05:32.699><c> of</c><00:05:33.300><c> computer</c> sophistication of of computer sophistication of of computer intelligence<00:05:34.259><c> is</c> intelligence is intelligence is growing<00:05:36.120><c> dramatically</c><00:05:36.840><c> I</c><00:05:37.440><c> mean</c><00:05:37.680><c> I</c><00:05:37.800><c> really</c> growing dramatically I mean I really growing dramatically I mean I really think<00:05:38.100><c> we're</c><00:05:38.280><c> on</c><00:05:38.520><c> an</c><00:05:38.699><c> exponential</c> think we're on an exponential think we're on an exponential Improvement<00:05:41.460><c> path</c><00:05:42.000><c> of</c><00:05:42.620><c> artificial</c> Improvement path of artificial Improvement path of artificial intelligence<00:05:44.100><c> and</c><00:05:44.460><c> the</c><00:05:44.520><c> number</c><00:05:44.880><c> of</c><00:05:45.060><c> smart</c> intelligence and the number of smart intelligence and the number of smart humans<00:05:45.840><c> that</c><00:05:46.199><c> are</c><00:05:46.380><c> developing</c><00:05:46.740><c> AI</c><00:05:47.100><c> is</c><00:05:47.280><c> also</c> humans that are developing AI is also humans that are developing AI is also increasing<00:05:48.000><c> dramatically</c><00:05:48.479><c> I</c><00:05:49.199><c> mean</c><00:05:49.320><c> if</c><00:05:49.440><c> you</c> increasing dramatically I mean if you increasing dramatically I mean if you look<00:05:49.680><c> at</c><00:05:49.740><c> like</c><00:05:49.979><c> the</c><00:05:50.100><c> attendance</c><00:05:50.580><c> at</c><00:05:51.120><c> the</c><00:05:51.500><c> AI</c> look at like the attendance at the AI look at like the attendance at the AI conferences<00:05:53.160><c> they're</c><00:05:54.000><c> they're</c><00:05:54.180><c> doubling</c> conferences they're they're doubling conferences they're they're doubling every<00:05:54.840><c> year</c><00:05:55.460><c> they're</c><00:05:56.460><c> getting</c><00:05:56.639><c> full</c> every year they're getting full every year they're getting full I<00:05:59.460><c> have</c><00:05:59.639><c> a</c><00:05:59.940><c> a</c><00:06:00.360><c> sort</c><00:06:01.320><c> of</c><00:06:01.440><c> a</c><00:06:01.500><c> young</c><00:06:01.620><c> cousin</c><00:06:01.919><c> of</c> I have a a sort of a young cousin of I have a a sort of a young cousin of mine<00:06:02.160><c> who's</c><00:06:02.400><c> graduating</c><00:06:02.880><c> from</c><00:06:03.060><c> Berkeley</c> mine who's graduating from Berkeley mine who's graduating from Berkeley um<00:06:04.919><c> in</c><00:06:05.520><c> computer</c><00:06:05.759><c> science</c><00:06:06.060><c> and</c><00:06:06.240><c> physics</c><00:06:06.539><c> and</c> um in computer science and physics and um in computer science and physics and I<00:06:07.919><c> asked</c><00:06:08.100><c> him</c><00:06:08.220><c> like</c><00:06:08.580><c> well</c><00:06:09.180><c> how</c> I asked him like well how I asked him like well how many<00:06:11.280><c> of</c><00:06:11.580><c> the</c><00:06:11.759><c> smart</c><00:06:12.000><c> students</c><00:06:12.840><c> are</c><00:06:13.500><c> studying</c> many of the smart students are studying many of the smart students are studying AI<00:06:14.400><c> in</c><00:06:14.639><c> computer</c><00:06:14.820><c> science</c><00:06:15.240><c> and</c><00:06:15.600><c> the</c><00:06:15.720><c> answer</c><00:06:15.840><c> is</c> AI in computer science and the answer is AI in computer science and the answer is all<00:06:16.199><c> of</c><00:06:16.320><c> them</c> all of them all of them what<00:06:18.240><c> were</c><00:06:18.419><c> they</c> what were they what were they okay<00:06:20.000><c> so</c><00:06:21.000><c> I</c><00:06:21.240><c> want</c><00:06:21.360><c> to</c><00:06:21.539><c> touch</c><00:06:21.720><c> on</c><00:06:21.900><c> this</c><00:06:22.139><c> because</c> okay so I want to touch on this because okay so I want to touch on this because I<00:06:23.819><c> agree</c><00:06:24.120><c> that</c><00:06:24.360><c> more</c><00:06:24.600><c> and</c><00:06:24.720><c> more</c><00:06:24.900><c> people</c><00:06:25.020><c> are</c> I agree that more and more people are I agree that more and more people are going<00:06:25.560><c> into</c><00:06:25.860><c> AI</c><00:06:26.759><c> research</c><00:06:27.300><c> and</c> going into AI research and going into AI research and studying<00:06:29.460><c> a</c><00:06:29.580><c> lot</c><00:06:29.699><c> of</c><00:06:29.819><c> things</c> studying a lot of things studying a lot of things but but but something something something that<00:06:34.680><c> Elon</c><00:06:35.100><c> doesn't</c><00:06:35.460><c> really</c><00:06:35.759><c> get</c><00:06:36.000><c> into</c><00:06:36.180><c> here</c> that Elon doesn't really get into here that Elon doesn't really get into here that<00:06:36.900><c> I</c><00:06:37.080><c> think</c><00:06:37.199><c> is</c><00:06:37.620><c> commonly</c><00:06:38.539><c> ignored</c><00:06:39.539><c> is</c><00:06:40.319><c> the</c> that I think is commonly ignored is the that I think is commonly ignored is the fact<00:06:40.800><c> that</c><00:06:41.160><c> while</c><00:06:41.639><c> we</c><00:06:41.880><c> are</c> fact that while we are fact that while we are kind<00:06:44.280><c> of</c><00:06:44.460><c> uh</c><00:06:45.240><c> exponentially</c><00:06:46.100><c> improving</c><00:06:47.100><c> our</c> kind of uh exponentially improving our kind of uh exponentially improving our AI<00:06:48.300><c> algorithms</c><00:06:48.840><c> and</c><00:06:48.960><c> everything</c><00:06:49.139><c> like</c><00:06:49.500><c> that</c> AI algorithms and everything like that AI algorithms and everything like that um um um I<00:06:52.560><c> don't</c><00:06:52.680><c> think</c><00:06:52.800><c> he's</c><00:06:52.860><c> accounting</c><00:06:53.280><c> for</c><00:06:53.400><c> the</c> I don't think he's accounting for the I don't think he's accounting for the fact<00:06:53.819><c> that</c><00:06:54.240><c> people</c><00:06:54.840><c> as</c><00:06:55.080><c> a</c><00:06:55.259><c> whole</c><00:06:55.380><c> are</c><00:06:55.860><c> also</c> fact that people as a whole are also fact that people as a whole are also getting<00:06:56.940><c> exponentially</c><00:06:57.600><c> smarter</c> getting exponentially smarter getting exponentially smarter if<00:06:59.220><c> you</c><00:06:59.340><c> think</c><00:06:59.520><c> about</c><00:06:59.759><c> what</c><00:07:00.360><c> it</c><00:07:00.539><c> took</c><00:07:00.840><c> to</c><00:07:01.380><c> be</c> if you think about what it took to be if you think about what it took to be able<00:07:01.680><c> to</c><00:07:02.280><c> change</c><00:07:02.759><c> careers</c> able to change careers able to change careers 20<00:07:04.979><c> 30</c><00:07:05.280><c> 40</c><00:07:05.580><c> years</c><00:07:05.880><c> ago</c><00:07:06.500><c> there's</c><00:07:07.500><c> a</c><00:07:07.800><c> lot</c><00:07:07.919><c> that</c> 20 30 40 years ago there's a lot that 20 30 40 years ago there's a lot that goes<00:07:08.340><c> into</c><00:07:08.460><c> it</c><00:07:08.819><c> you</c><00:07:09.060><c> know</c><00:07:09.120><c> typically</c><00:07:09.539><c> you've</c> goes into it you know typically you've goes into it you know typically you've got<00:07:10.020><c> four-year</c><00:07:10.500><c> degrees</c><00:07:10.740><c> that</c><00:07:11.100><c> are</c><00:07:11.220><c> required</c> got four-year degrees that are required got four-year degrees that are required for<00:07:11.880><c> very</c><00:07:12.120><c> specific</c><00:07:12.479><c> things</c><00:07:12.840><c> you</c><00:07:13.800><c> go</c><00:07:14.039><c> in</c> for very specific things you go in for very specific things you go in learning<00:07:15.240><c> is</c><00:07:15.419><c> a</c><00:07:15.660><c> very</c><00:07:15.780><c> slow</c><00:07:16.020><c> process</c> learning is a very slow process learning is a very slow process however<00:07:17.940><c> we</c><00:07:18.479><c> now</c><00:07:18.600><c> have</c><00:07:18.900><c> access</c><00:07:19.139><c> to</c><00:07:19.919><c> all</c><00:07:20.340><c> of</c><00:07:20.460><c> the</c> however we now have access to all of the however we now have access to all of the world's<00:07:20.819><c> information</c><00:07:21.000><c> at</c><00:07:21.479><c> our</c><00:07:21.599><c> fingertips</c> world's information at our fingertips world's information at our fingertips and<00:07:24.139><c> so</c><00:07:25.139><c> it's</c><00:07:25.560><c> becoming</c><00:07:26.099><c> much</c><00:07:26.340><c> more</c><00:07:26.639><c> common</c> and so it's becoming much more common and so it's becoming much more common that<00:07:27.360><c> people</c><00:07:27.599><c> are</c><00:07:28.160><c> multi-disciplinary</c><00:07:29.160><c> in</c> that people are multi-disciplinary in that people are multi-disciplinary in nature<00:07:29.960><c> and</c><00:07:30.960><c> so</c><00:07:31.380><c> yes</c><00:07:31.560><c> I'm</c><00:07:31.860><c> a</c><00:07:32.039><c> computer</c> nature and so yes I'm a computer nature and so yes I'm a computer programmer<00:07:32.819><c> and</c><00:07:33.180><c> now</c><00:07:33.360><c> I'm</c><00:07:33.539><c> also</c> programmer and now I'm also programmer and now I'm also you<00:07:35.520><c> know</c><00:07:35.580><c> becoming</c><00:07:36.360><c> a</c><00:07:36.840><c> YouTuber</c><00:07:37.680><c> and</c><00:07:37.919><c> a</c> you know becoming a YouTuber and a you know becoming a YouTuber and a podcaster<00:07:38.520><c> and</c><00:07:38.699><c> a</c><00:07:38.880><c> writer</c><00:07:39.560><c> and</c><00:07:40.560><c> I'm</c><00:07:41.340><c> learning</c> podcaster and a writer and I'm learning podcaster and a writer and I'm learning from<00:07:42.720><c> all</c><00:07:43.139><c> sorts</c><00:07:43.380><c> of</c><00:07:43.440><c> different</c><00:07:43.680><c> domains</c><00:07:44.400><c> and</c> from all sorts of different domains and from all sorts of different domains and I'm<00:07:45.240><c> pulling</c><00:07:45.599><c> that</c><00:07:45.780><c> together</c><00:07:46.580><c> in</c><00:07:47.580><c> what</c><00:07:48.300><c> I</c> I'm pulling that together in what I I'm pulling that together in what I believe<00:07:48.599><c> are</c><00:07:48.900><c> novel</c><00:07:49.440><c> ways</c><00:07:49.919><c> and</c><00:07:50.539><c> applying</c><00:07:51.539><c> this</c> believe are novel ways and applying this believe are novel ways and applying this knowledge<00:07:52.259><c> in</c><00:07:52.979><c> a</c><00:07:53.340><c> variety</c><00:07:53.759><c> of</c><00:07:54.000><c> different</c><00:07:54.180><c> ways</c> knowledge in a variety of different ways knowledge in a variety of different ways and<00:07:55.860><c> so</c> and so and so while<00:07:58.139><c> our</c><00:07:58.560><c> tools</c><00:07:58.979><c> are</c><00:07:59.280><c> in</c><00:07:59.460><c> fact</c><00:07:59.580><c> becoming</c> while our tools are in fact becoming while our tools are in fact becoming better<00:08:00.240><c> I</c><00:08:00.900><c> think</c><00:08:00.960><c> our</c><00:08:01.199><c> ability</c><00:08:01.500><c> to</c><00:08:01.800><c> apply</c> better I think our ability to apply better I think our ability to apply those<00:08:02.400><c> tools</c><00:08:02.819><c> is</c><00:08:03.419><c> also</c><00:08:03.840><c> increasing</c><00:08:04.440><c> alongside</c> those tools is also increasing alongside those tools is also increasing alongside that<00:08:05.599><c> and</c><00:08:06.599><c> if</c><00:08:07.080><c> we</c><00:08:07.259><c> can</c><00:08:07.380><c> use</c><00:08:07.680><c> this</c><00:08:08.099><c> in</c><00:08:08.699><c> a</c><00:08:09.000><c> very</c> that and if we can use this in a very that and if we can use this in a very productive<00:08:09.660><c> helpful</c><00:08:10.380><c> way</c> productive helpful way productive helpful way and<00:08:12.000><c> use</c><00:08:12.360><c> it</c><00:08:12.539><c> to</c><00:08:12.720><c> help</c><00:08:12.840><c> fix</c><00:08:13.080><c> a</c><00:08:13.440><c> lot</c><00:08:13.500><c> of</c><00:08:13.620><c> the</c> and use it to help fix a lot of the and use it to help fix a lot of the problems<00:08:13.979><c> with</c><00:08:14.160><c> Society</c><00:08:14.660><c> I</c><00:08:15.660><c> think</c><00:08:15.840><c> then</c><00:08:16.800><c> it</c> problems with Society I think then it problems with Society I think then it becomes<00:08:17.580><c> this</c> becomes this becomes this symbiotic<00:08:20.160><c> relationship</c><00:08:20.639><c> between</c><00:08:21.240><c> us</c><00:08:21.780><c> and</c><00:08:22.139><c> Ai</c> symbiotic relationship between us and Ai symbiotic relationship between us and Ai and<00:08:23.220><c> this</c><00:08:23.460><c> is</c><00:08:23.580><c> ultimately</c><00:08:24.139><c> what</c><00:08:25.139><c> I'm</c><00:08:25.259><c> hoping</c> and this is ultimately what I'm hoping and this is ultimately what I'm hoping to<00:08:25.740><c> shoot</c><00:08:25.919><c> for</c><00:08:26.479><c> uh</c><00:08:27.479><c> especially</c><00:08:28.199><c> with</c> to shoot for uh especially with to shoot for uh especially with everything<00:08:28.500><c> I'm</c><00:08:28.740><c> doing</c><00:08:28.919><c> I'm</c><00:08:29.099><c> looking</c><00:08:29.340><c> for</c> everything I'm doing I'm looking for everything I'm doing I'm looking for ways<00:08:29.819><c> to</c><00:08:30.000><c> introduce</c><00:08:30.300><c> AI</c><00:08:30.780><c> into</c><00:08:31.520><c> different</c> ways to introduce AI into different ways to introduce AI into different people's people's people's toolbox<00:08:34.820><c> in</c><00:08:35.820><c> kind</c><00:08:36.120><c> of</c><00:08:36.180><c> new</c><00:08:36.419><c> and</c><00:08:36.599><c> exciting</c><00:08:36.959><c> ways</c> toolbox in kind of new and exciting ways toolbox in kind of new and exciting ways things<00:08:37.680><c> that</c><00:08:37.979><c> weren't</c><00:08:38.279><c> previously</c><00:08:38.820><c> available</c> things that weren't previously available things that weren't previously available to<00:08:39.539><c> us</c><00:08:39.659><c> ways</c><00:08:40.560><c> to</c><00:08:40.740><c> unlock</c><00:08:40.919><c> information</c><00:08:41.520><c> and</c> to us ways to unlock information and to us ways to unlock information and knowledge<00:08:42.659><c> in</c><00:08:43.440><c> new</c><00:08:43.979><c> and</c><00:08:44.159><c> exciting</c><00:08:44.459><c> ways</c> knowledge in new and exciting ways knowledge in new and exciting ways because<00:08:45.200><c> it's</c><00:08:46.200><c> how</c><00:08:46.800><c> all</c><00:08:47.160><c> of</c><00:08:47.279><c> this</c><00:08:47.459><c> technology</c> because it's how all of this technology because it's how all of this technology gets<00:08:48.360><c> applied</c> gets applied gets applied um<00:08:49.980><c> you</c><00:08:50.160><c> know</c><00:08:50.279><c> I</c><00:08:50.459><c> don't</c><00:08:50.580><c> think</c><00:08:50.700><c> once</c><00:08:51.240><c> AI</c><00:08:51.779><c> gets</c> um you know I don't think once AI gets um you know I don't think once AI gets to<00:08:52.320><c> a</c><00:08:52.500><c> certain</c><00:08:52.560><c> level</c><00:08:52.740><c> it's</c><00:08:53.040><c> just</c><00:08:53.220><c> going</c><00:08:53.339><c> to</c> to a certain level it's just going to to a certain level it's just going to fix<00:08:53.580><c> all</c><00:08:53.820><c> of</c><00:08:53.940><c> our</c><00:08:54.060><c> problems</c><00:08:54.420><c> for</c><00:08:54.600><c> us</c><00:08:54.920><c> it's</c> fix all of our problems for us it's fix all of our problems for us it's still<00:08:56.339><c> going</c><00:08:56.519><c> to</c><00:08:56.700><c> rely</c><00:08:57.060><c> a</c><00:08:57.240><c> lot</c><00:08:57.360><c> on</c> still going to rely a lot on still going to rely a lot on understanding<00:08:58.860><c> what</c><00:08:59.339><c> our</c><00:08:59.519><c> problems</c><00:09:00.000><c> actually</c> understanding what our problems actually understanding what our problems actually are<00:09:00.600><c> and</c><00:09:01.440><c> I</c><00:09:01.560><c> think</c><00:09:01.680><c> there</c><00:09:01.860><c> are</c><00:09:01.980><c> a</c><00:09:02.040><c> lot</c><00:09:02.100><c> of</c> are and I think there are a lot of are and I think there are a lot of people<00:09:02.339><c> who</c><00:09:02.700><c> don't</c><00:09:02.880><c> actually</c><00:09:03.240><c> understand</c> people who don't actually understand people who don't actually understand what<00:09:04.260><c> a</c><00:09:04.440><c> lot</c><00:09:04.500><c> of</c><00:09:04.620><c> Humanity's</c><00:09:05.100><c> problems</c><00:09:05.399><c> are</c> what a lot of Humanity's problems are what a lot of Humanity's problems are they<00:09:07.080><c> have</c><00:09:07.320><c> their</c><00:09:07.560><c> view</c><00:09:07.680><c> of</c><00:09:07.860><c> the</c><00:09:08.040><c> world</c><00:09:08.220><c> and</c> they have their view of the world and they have their view of the world and they<00:09:09.480><c> have</c><00:09:09.600><c> their</c><00:09:09.839><c> framing</c><00:09:10.260><c> of</c><00:09:10.380><c> what</c><00:09:10.560><c> the</c> they have their framing of what the they have their framing of what the problem<00:09:10.920><c> is</c><00:09:11.220><c> but</c><00:09:12.120><c> a</c><00:09:12.360><c> lot</c><00:09:12.420><c> of</c><00:09:12.480><c> times</c><00:09:12.600><c> it</c><00:09:12.839><c> doesn't</c> problem is but a lot of times it doesn't problem is but a lot of times it doesn't actually<00:09:13.200><c> get</c><00:09:13.500><c> down</c><00:09:13.740><c> to</c><00:09:14.100><c> the</c><00:09:14.339><c> root</c><00:09:14.700><c> cause</c><00:09:14.940><c> of</c> actually get down to the root cause of actually get down to the root cause of problems<00:09:16.160><c> it</c><00:09:17.160><c> is</c><00:09:17.339><c> a</c><00:09:18.180><c> very</c> problems it is a very problems it is a very specifically<00:09:20.640><c> colored</c><00:09:21.120><c> view</c><00:09:21.360><c> of</c><00:09:21.660><c> those</c> specifically colored view of those specifically colored view of those problems<00:09:22.560><c> and</c><00:09:23.279><c> the</c><00:09:23.519><c> people</c><00:09:23.640><c> talking</c><00:09:23.940><c> about</c><00:09:24.180><c> it</c> problems and the people talking about it problems and the people talking about it for<00:09:25.560><c> the</c><00:09:25.620><c> most</c><00:09:25.800><c> part</c><00:09:25.980><c> are</c><00:09:26.279><c> people</c><00:09:26.519><c> who</c><00:09:26.700><c> are</c> for the most part are people who are for the most part are people who are incentivized<00:09:27.540><c> to</c><00:09:27.839><c> talk</c><00:09:28.140><c> about</c><00:09:28.459><c> problems</c><00:09:29.459><c> in</c> incentivized to talk about problems in incentivized to talk about problems in ways<00:09:30.060><c> that</c><00:09:30.240><c> get</c><00:09:30.420><c> people</c><00:09:30.540><c> riled</c><00:09:31.019><c> up</c><00:09:31.140><c> and</c><00:09:31.500><c> start</c> ways that get people riled up and start ways that get people riled up and start drawing<00:09:32.399><c> money</c><00:09:32.580><c> at</c><00:09:32.880><c> the</c><00:09:33.060><c> problem</c><00:09:33.260><c> or</c><00:09:34.260><c> the</c> drawing money at the problem or the drawing money at the problem or the person<00:09:34.740><c> talking</c><00:09:35.100><c> about</c><00:09:35.279><c> the</c><00:09:35.580><c> problem</c> person talking about the problem person talking about the problem depending<00:09:36.360><c> on</c><00:09:36.540><c> exactly</c><00:09:36.959><c> how</c><00:09:37.320><c> that's</c><00:09:37.500><c> set</c><00:09:37.740><c> up</c> depending on exactly how that's set up depending on exactly how that's set up and<00:09:38.820><c> so</c><00:09:39.000><c> a</c><00:09:39.120><c> lot</c><00:09:39.240><c> of</c><00:09:39.300><c> these</c><00:09:39.420><c> incentive</c> and so a lot of these incentive and so a lot of these incentive structures<00:09:40.200><c> are</c><00:09:40.680><c> weird</c><00:09:41.000><c> but</c><00:09:42.000><c> I</c><00:09:42.240><c> think</c><00:09:42.300><c> we</c><00:09:42.480><c> can</c> structures are weird but I think we can structures are weird but I think we can get<00:09:42.720><c> to</c><00:09:42.779><c> a</c><00:09:42.959><c> place</c><00:09:43.140><c> where</c><00:09:43.440><c> the</c><00:09:43.680><c> incentive</c> get to a place where the incentive get to a place where the incentive structures<00:09:44.339><c> align</c><00:09:44.700><c> with</c><00:09:44.880><c> the</c><00:09:45.000><c> actual</c><00:09:45.240><c> solving</c> structures align with the actual solving structures align with the actual solving of<00:09:45.839><c> the</c><00:09:46.019><c> problems</c><00:09:46.320><c> as</c><00:09:46.620><c> opposed</c><00:09:46.980><c> to</c><00:09:47.040><c> the</c> of the problems as opposed to the of the problems as opposed to the broadcasting<00:09:48.899><c> of</c><00:09:49.080><c> the</c><00:09:49.260><c> problems</c><00:09:49.740><c> and</c><00:09:50.459><c> this</c><00:09:50.820><c> is</c> broadcasting of the problems and this is broadcasting of the problems and this is the<00:09:51.060><c> shift</c><00:09:51.240><c> that</c><00:09:51.480><c> we</c><00:09:51.660><c> need</c><00:09:51.839><c> to</c><00:09:52.019><c> hit</c><00:09:52.140><c> I</c><00:09:52.500><c> think</c> the shift that we need to hit I think the shift that we need to hit I think pretty<00:09:53.580><c> quickly</c><00:09:54.000><c> because</c><00:09:54.300><c> if</c><00:09:54.660><c> we</c><00:09:54.899><c> don't</c><00:09:55.160><c> again</c> pretty quickly because if we don't again pretty quickly because if we don't again that's<00:09:56.459><c> where</c><00:09:56.700><c> the</c><00:09:56.880><c> humans</c><00:09:57.240><c> start</c><00:09:57.420><c> coming</c><00:09:57.660><c> in</c> that's where the humans start coming in that's where the humans start coming in and and and becoming<00:10:00.120><c> the</c><00:10:00.420><c> problem</c> becoming the problem becoming the problem that<00:10:02.160><c> we</c><00:10:02.339><c> currently</c><00:10:02.700><c> are</c><00:10:03.000><c> talking</c><00:10:03.180><c> about</c><00:10:03.480><c> AI</c> that we currently are talking about AI that we currently are talking about AI becoming becoming becoming let's<00:10:05.880><c> continue</c> let's continue let's continue a<00:10:07.019><c> better</c><00:10:07.580><c> approach</c><00:10:08.580><c> a</c><00:10:09.000><c> better</c><00:10:09.540><c> outcome</c><00:10:09.980><c> is</c> a better approach a better outcome is a better approach a better outcome is that<00:10:12.060><c> we</c><00:10:12.540><c> achieve</c><00:10:12.839><c> democratization</c><00:10:13.560><c> of</c><00:10:13.920><c> AI</c> that we achieve democratization of AI that we achieve democratization of AI technology<00:10:14.700><c> meaning</c><00:10:15.060><c> that</c><00:10:15.740><c> no</c><00:10:16.740><c> one</c><00:10:16.980><c> company</c> technology meaning that no one company technology meaning that no one company or<00:10:18.320><c> a</c><00:10:19.320><c> small</c><00:10:19.620><c> set</c><00:10:19.860><c> of</c><00:10:19.980><c> individuals</c><00:10:20.339><c> has</c> or a small set of individuals has or a small set of individuals has control<00:10:20.820><c> over</c><00:10:21.180><c> Advanced</c><00:10:21.720><c> AI</c><00:10:22.019><c> technology</c><00:10:22.500><c> like</c> control over Advanced AI technology like control over Advanced AI technology like that<00:10:23.580><c> I</c><00:10:23.700><c> agree</c><00:10:23.880><c> with</c><00:10:24.060><c> that</c><00:10:24.180><c> that's</c><00:10:24.360><c> very</c> that I agree with that that's very that I agree with that that's very dangerous dangerous dangerous [Music] [Music] [Music] it<00:10:27.779><c> closer</c><00:10:28.200><c> gets</c><00:10:28.440><c> stolen</c><00:10:28.680><c> by</c><00:10:28.920><c> somebody</c><00:10:29.100><c> bad</c> it closer gets stolen by somebody bad it closer gets stolen by somebody bad you<00:10:29.940><c> know</c><00:10:30.240><c> like</c><00:10:30.720><c> some</c><00:10:30.899><c> evil</c><00:10:31.200><c> dictator</c><00:10:31.560><c> the</c> you know like some evil dictator the you know like some evil dictator the country<00:10:32.339><c> could</c><00:10:32.700><c> send</c><00:10:32.940><c> their</c><00:10:33.360><c> intelligence</c> country could send their intelligence country could send their intelligence agency<00:10:34.620><c> to</c><00:10:34.920><c> go</c><00:10:35.040><c> steal</c><00:10:35.279><c> it</c><00:10:35.459><c> and</c><00:10:35.640><c> gain</c><00:10:35.880><c> control</c> agency to go steal it and gain control agency to go steal it and gain control it<00:10:36.899><c> just</c><00:10:37.019><c> becomes</c><00:10:37.320><c> a</c><00:10:37.500><c> very</c><00:10:37.620><c> unstable</c> it just becomes a very unstable it just becomes a very unstable situation<00:10:38.519><c> I</c><00:10:38.760><c> think</c><00:10:38.880><c> if</c><00:10:39.060><c> you've</c><00:10:39.240><c> got</c><00:10:39.420><c> any</c> situation I think if you've got any situation I think if you've got any um um um any<00:10:41.880><c> incredibly</c><00:10:42.300><c> powerful</c><00:10:42.720><c> AI</c> any incredibly powerful AI any incredibly powerful AI um um um you<00:10:45.480><c> just</c><00:10:45.600><c> don't</c><00:10:45.779><c> know</c><00:10:45.899><c> who's</c><00:10:46.200><c> who's</c><00:10:46.680><c> going</c><00:10:46.920><c> to</c> you just don't know who's who's going to you just don't know who's who's going to control<00:10:47.100><c> that</c><00:10:47.399><c> so</c><00:10:47.700><c> it's</c><00:10:48.240><c> not</c><00:10:48.540><c> as</c><00:10:48.660><c> I</c><00:10:48.839><c> think</c><00:10:48.959><c> that</c> control that so it's not as I think that control that so it's not as I think that the<00:10:49.200><c> risk</c><00:10:49.440><c> is</c><00:10:49.560><c> that</c><00:10:49.740><c> the</c><00:10:49.860><c> AI</c><00:10:50.100><c> would</c><00:10:50.339><c> develop</c><00:10:50.579><c> a</c> the risk is that the AI would develop a the risk is that the AI would develop a will<00:10:50.880><c> of</c><00:10:51.000><c> its</c><00:10:51.180><c> own</c><00:10:51.240><c> right</c><00:10:51.480><c> off</c><00:10:51.600><c> the</c><00:10:51.779><c> bat</c><00:10:51.899><c> I</c> will of its own right off the bat I will of its own right off the bat I think<00:10:52.320><c> it's</c><00:10:52.860><c> more</c><00:10:53.040><c> that's</c><00:10:53.579><c> the</c><00:10:54.000><c> concern</c><00:10:54.300><c> is</c> think it's more that's the concern is think it's more that's the concern is that<00:10:54.779><c> some</c><00:10:55.500><c> someone</c> that some someone that some someone um<00:10:57.300><c> may</c><00:10:57.779><c> use</c><00:10:58.019><c> it</c><00:10:58.140><c> in</c><00:10:58.320><c> a</c><00:10:58.440><c> way</c><00:10:58.560><c> that</c><00:10:58.740><c> is</c><00:10:58.920><c> bad</c><00:10:59.420><c> or</c> um may use it in a way that is bad or um may use it in a way that is bad or and<00:11:00.839><c> even</c><00:11:01.079><c> if</c><00:11:01.260><c> they</c><00:11:01.440><c> weren't</c><00:11:01.620><c> going</c><00:11:01.740><c> to</c><00:11:01.800><c> use</c><00:11:01.920><c> it</c> and even if they weren't going to use it and even if they weren't going to use it in<00:11:02.100><c> a</c><00:11:02.220><c> way</c><00:11:02.339><c> that's</c><00:11:02.459><c> bad</c><00:11:02.640><c> that</c><00:11:02.880><c> somebody</c><00:11:03.060><c> could</c> in a way that's bad that somebody could in a way that's bad that somebody could take<00:11:03.540><c> it</c><00:11:03.720><c> from</c><00:11:03.959><c> them</c><00:11:04.079><c> and</c><00:11:04.320><c> use</c><00:11:04.500><c> it</c><00:11:05.100><c> sorry</c><00:11:05.579><c> I</c> take it from them and use it sorry I take it from them and use it sorry I just<00:11:06.060><c> need</c><00:11:06.240><c> to</c><00:11:06.360><c> comment</c><00:11:06.720><c> on</c><00:11:07.620><c> the</c><00:11:08.160><c> the</c><00:11:08.399><c> hacker</c> just need to comment on the the hacker just need to comment on the the hacker in<00:11:09.060><c> the</c><00:11:09.180><c> V</c><00:11:09.300><c> for</c><00:11:09.480><c> Vendetta</c><00:11:10.019><c> mask</c> in the V for Vendetta mask in the V for Vendetta mask um um um yes that's<00:11:18.540><c> entirely</c><00:11:19.140><c> ridiculous</c> that's entirely ridiculous that's entirely ridiculous um<00:11:20.459><c> sorry</c><00:11:21.120><c> that</c><00:11:21.360><c> imagery</c><00:11:21.779><c> uh</c><00:11:22.560><c> kind</c><00:11:22.740><c> of</c><00:11:22.860><c> kills</c> um sorry that imagery uh kind of kills um sorry that imagery uh kind of kills me<00:11:23.160><c> here</c><00:11:23.339><c> but</c> me here but me here but I<00:11:25.140><c> I</c><00:11:25.440><c> do</c><00:11:25.800><c> think</c><00:11:25.980><c> this</c><00:11:26.160><c> is</c><00:11:26.279><c> an</c><00:11:26.399><c> important</c><00:11:26.700><c> point</c> I I do think this is an important point I I do think this is an important point of<00:11:27.600><c> not</c><00:11:28.140><c> one</c><00:11:28.500><c> person</c><00:11:28.620><c> or</c><00:11:28.860><c> one</c><00:11:29.040><c> country</c> of not one person or one country of not one person or one country controlling controlling controlling um<00:11:31.079><c> AI</c><00:11:31.680><c> I</c><00:11:31.980><c> think</c><00:11:32.040><c> this</c><00:11:32.220><c> is</c><00:11:32.339><c> a</c><00:11:32.519><c> big</c><00:11:32.640><c> reason</c><00:11:32.940><c> why</c> um AI I think this is a big reason why um AI I think this is a big reason why uh<00:11:34.500><c> open</c><00:11:34.680><c> source</c><00:11:35.040><c> models</c><00:11:35.399><c> are</c><00:11:35.640><c> so</c><00:11:35.820><c> important</c> uh open source models are so important uh open source models are so important and<00:11:36.480><c> a</c><00:11:36.600><c> lot</c><00:11:36.720><c> of</c><00:11:36.779><c> things</c><00:11:36.959><c> like</c> and a lot of things like and a lot of things like um the<00:11:38.279><c> stuff</c><00:11:38.459><c> that</c><00:11:38.760><c> hugging</c><00:11:39.300><c> face</c><00:11:39.480><c> is</c><00:11:39.720><c> doing</c> um the stuff that hugging face is doing um the stuff that hugging face is doing to<00:11:40.140><c> support</c><00:11:40.320><c> open</c><00:11:40.680><c> models</c> to support open models to support open models um um um if<00:11:43.380><c> you</c><00:11:43.560><c> listen</c><00:11:43.680><c> to</c><00:11:43.800><c> a</c><00:11:43.920><c> lot</c><00:11:43.980><c> of</c><00:11:44.100><c> what</c><00:11:44.160><c> I</c><00:11:44.339><c> talk</c> if you listen to a lot of what I talk if you listen to a lot of what I talk about<00:11:44.579><c> I'm</c><00:11:44.880><c> a</c><00:11:45.060><c> big</c><00:11:45.180><c> believer</c><00:11:45.480><c> in</c><00:11:45.779><c> the</c><00:11:45.959><c> idea</c><00:11:46.140><c> of</c> about I'm a big believer in the idea of about I'm a big believer in the idea of Open<00:11:46.500><c> Source</c><00:11:46.980><c> software</c><00:11:47.339><c> and</c><00:11:47.579><c> I</c><00:11:47.700><c> think</c><00:11:47.760><c> we</c><00:11:47.940><c> need</c> Open Source software and I think we need Open Source software and I think we need to<00:11:48.180><c> get</c><00:11:48.300><c> to</c><00:11:48.480><c> this</c><00:11:48.720><c> point</c><00:11:49.380><c> where</c><00:11:49.740><c> things</c><00:11:49.980><c> are</c> to get to this point where things are to get to this point where things are more<00:11:50.579><c> and</c><00:11:50.700><c> more</c><00:11:50.940><c> open</c> more and more open more and more open um um um because<00:11:54.860><c> it</c><00:11:55.860><c> does</c><00:11:56.040><c> level</c><00:11:56.339><c> the</c><00:11:56.640><c> playing</c><00:11:56.760><c> field</c> because it does level the playing field because it does level the playing field and<00:11:57.300><c> kind</c><00:11:57.480><c> of</c><00:11:57.540><c> give</c><00:11:57.660><c> everybody</c><00:11:57.959><c> access</c><00:11:58.440><c> to</c><00:11:59.160><c> the</c> and kind of give everybody access to the and kind of give everybody access to the same<00:12:00.120><c> thing</c> same thing same thing um um um I<00:12:03.959><c> I</c><00:12:03.959><c> think</c> I I think I I think with<00:12:06.540><c> AI</c><00:12:06.899><c> there</c><00:12:07.320><c> does</c><00:12:07.500><c> become</c><00:12:07.920><c> quite</c><00:12:08.220><c> The</c> with AI there does become quite The with AI there does become quite The Balancing<00:12:08.880><c> Act</c><00:12:09.120><c> much</c><00:12:09.839><c> in</c><00:12:10.079><c> the</c><00:12:10.200><c> same</c><00:12:10.380><c> way</c> Balancing Act much in the same way Balancing Act much in the same way um<00:12:11.940><c> like</c><00:12:12.360><c> when</c><00:12:12.540><c> the</c><00:12:12.899><c> US</c><00:12:13.019><c> developed</c><00:12:13.560><c> nuclear</c> um like when the US developed nuclear um like when the US developed nuclear weapons<00:12:14.279><c> and</c><00:12:14.519><c> then</c><00:12:14.760><c> you</c><00:12:15.240><c> know</c><00:12:15.360><c> Russia</c><00:12:15.720><c> was</c> weapons and then you know Russia was weapons and then you know Russia was able<00:12:16.079><c> to</c><00:12:16.200><c> get</c><00:12:16.320><c> them</c><00:12:16.560><c> sorry</c><00:12:17.040><c> Soviet</c><00:12:17.519><c> Union</c><00:12:17.700><c> was</c> able to get them sorry Soviet Union was able to get them sorry Soviet Union was able<00:12:18.180><c> to</c><00:12:18.240><c> get</c><00:12:18.360><c> them</c><00:12:18.480><c> shortly</c><00:12:18.779><c> thereafter</c><00:12:19.140><c> the</c> able to get them shortly thereafter the able to get them shortly thereafter the fact<00:12:20.100><c> that</c><00:12:20.519><c> both</c><00:12:21.240><c> countries</c><00:12:21.959><c> had</c><00:12:22.320><c> nuclear</c> fact that both countries had nuclear fact that both countries had nuclear weapons<00:12:22.980><c> kind</c><00:12:23.160><c> of</c><00:12:23.279><c> led</c><00:12:23.519><c> to</c><00:12:23.640><c> a</c><00:12:23.820><c> stalemate</c><00:12:24.540><c> right</c> weapons kind of led to a stalemate right weapons kind of led to a stalemate right you<00:12:24.959><c> don't</c><00:12:25.079><c> want</c><00:12:25.320><c> anyone</c><00:12:26.100><c> power</c><00:12:26.339><c> controlling</c> you don't want anyone power controlling you don't want anyone power controlling it it it um um um but<00:12:30.959><c> I</c><00:12:31.079><c> don't</c><00:12:31.200><c> think</c> but I don't think but I don't think government<00:12:33.600><c> is</c><00:12:34.200><c> necessarily</c><00:12:34.920><c> where</c><00:12:36.000><c> the</c><00:12:36.480><c> best</c> government is necessarily where the best government is necessarily where the best minds<00:12:36.899><c> are</c><00:12:37.019><c> going</c><00:12:37.140><c> right</c><00:12:37.440><c> now</c><00:12:37.620><c> and</c><00:12:38.220><c> so</c> minds are going right now and so minds are going right now and so one<00:12:40.380><c> of</c><00:12:40.440><c> the</c><00:12:40.620><c> podcasts</c><00:12:41.160><c> I</c><00:12:41.279><c> was</c><00:12:41.399><c> listening</c><00:12:41.640><c> to</c> one of the podcasts I was listening to one of the podcasts I was listening to the<00:12:41.880><c> other</c><00:12:42.000><c> day</c><00:12:42.660><c> I</c><00:12:42.660><c> was</c><00:12:43.019><c> talking</c><00:12:43.200><c> about</c><00:12:43.320><c> the</c> the other day I was talking about the the other day I was talking about the fact<00:12:43.620><c> that</c><00:12:43.800><c> it</c><00:12:43.920><c> used</c><00:12:44.040><c> to</c><00:12:44.160><c> be</c><00:12:44.220><c> that</c><00:12:44.399><c> the</c><00:12:44.519><c> best</c> fact that it used to be that the best fact that it used to be that the best and<00:12:44.760><c> brightest</c><00:12:45.120><c> would</c><00:12:45.240><c> go</c><00:12:45.420><c> work</c><00:12:45.620><c> in</c><00:12:46.620><c> the</c> and brightest would go work in the and brightest would go work in the government<00:12:47.040><c> they'd</c><00:12:47.459><c> go</c><00:12:47.519><c> work</c><00:12:47.760><c> for</c><00:12:48.000><c> NASA</c><00:12:48.600><c> in</c> government they'd go work for NASA in government they'd go work for NASA in the<00:12:48.899><c> US</c><00:12:49.019><c> or</c><00:12:49.500><c> you</c><00:12:50.459><c> know</c><00:12:50.519><c> the</c><00:12:50.760><c> the</c><00:12:50.959><c> Soviet</c><00:12:51.959><c> space</c> the US or you know the the Soviet space the US or you know the the Soviet space agency<00:12:52.680><c> or</c><00:12:52.980><c> things</c><00:12:53.399><c> like</c><00:12:53.700><c> that</c> agency or things like that agency or things like that um<00:12:55.200><c> but</c><00:12:55.800><c> right</c><00:12:55.920><c> now</c><00:12:56.100><c> we've</c><00:12:56.459><c> got</c><00:12:56.579><c> a</c><00:12:56.760><c> lot</c><00:12:56.940><c> more</c><00:12:57.560><c> of</c> um but right now we've got a lot more of um but right now we've got a lot more of the<00:12:58.740><c> smartest</c><00:12:59.100><c> people</c><00:12:59.279><c> going</c><00:12:59.639><c> into</c><00:12:59.880><c> private</c> the smartest people going into private the smartest people going into private Enterprise<00:13:00.860><c> that's</c><00:13:01.860><c> where</c><00:13:02.040><c> the</c><00:13:02.220><c> money</c><00:13:02.339><c> is</c> Enterprise that's where the money is Enterprise that's where the money is that's<00:13:02.700><c> where</c><00:13:02.880><c> the</c><00:13:03.000><c> impact</c><00:13:03.300><c> is</c> that's where the impact is that's where the impact is um<00:13:05.459><c> you</c><00:13:05.940><c> know</c><00:13:06.000><c> I</c><00:13:06.060><c> think</c><00:13:06.180><c> we</c><00:13:06.300><c> have</c><00:13:06.420><c> a</c><00:13:06.420><c> lot</c><00:13:06.540><c> of</c> um you know I think we have a lot of um you know I think we have a lot of people<00:13:06.720><c> in</c><00:13:06.899><c> the</c><00:13:07.019><c> world</c><00:13:07.139><c> who</c><00:13:07.500><c> want</c><00:13:07.800><c> to</c><00:13:07.980><c> be</c><00:13:08.100><c> known</c> people in the world who want to be known people in the world who want to be known for<00:13:08.519><c> what</c><00:13:08.639><c> they</c><00:13:08.760><c> do</c><00:13:08.880><c> they</c><00:13:09.120><c> want</c><00:13:09.240><c> to</c><00:13:09.360><c> have</c><00:13:09.480><c> a</c> for what they do they want to have a for what they do they want to have a meaningful<00:13:09.839><c> impact</c><00:13:10.260><c> they</c><00:13:10.680><c> want</c><00:13:10.980><c> to</c> meaningful impact they want to meaningful impact they want to immortalize<00:13:11.940><c> themselves</c><00:13:12.480><c> in</c><00:13:12.839><c> history</c><00:13:13.100><c> and</c> immortalize themselves in history and immortalize themselves in history and Elon<00:13:14.579><c> is</c><00:13:14.760><c> an</c><00:13:14.880><c> example</c><00:13:15.180><c> of</c><00:13:15.360><c> this</c><00:13:15.540><c> he</c><00:13:15.839><c> is</c><00:13:16.019><c> someone</c> Elon is an example of this he is someone Elon is an example of this he is someone who<00:13:17.339><c> wants</c><00:13:18.000><c> to</c><00:13:18.120><c> be</c><00:13:18.300><c> known</c><00:13:18.540><c> as</c><00:13:18.779><c> somebody</c><00:13:19.079><c> who</c> who wants to be known as somebody who who wants to be known as somebody who did<00:13:19.680><c> stuff</c><00:13:20.100><c> that</c><00:13:20.720><c> changed</c><00:13:21.720><c> human</c><00:13:22.260><c> history</c> did stuff that changed human history did stuff that changed human history um<00:13:24.480><c> and</c><00:13:25.200><c> the</c><00:13:25.440><c> fact</c><00:13:25.560><c> of</c><00:13:25.680><c> the</c><00:13:25.740><c> matter</c><00:13:25.920><c> is</c><00:13:26.040><c> we</c> um and the fact of the matter is we um and the fact of the matter is we individuals<00:13:26.880><c> do</c><00:13:27.120><c> have</c><00:13:27.300><c> that</c><00:13:27.540><c> power</c> individuals do have that power individuals do have that power um<00:13:29.579><c> and</c><00:13:30.360><c> I</c><00:13:30.899><c> think</c> um and I think um and I think hopefully<00:13:32.639><c> one</c><00:13:32.820><c> of</c><00:13:33.000><c> the</c><00:13:33.060><c> things</c><00:13:33.180><c> that</c><00:13:33.300><c> will</c> hopefully one of the things that will hopefully one of the things that will drive<00:13:33.600><c> people</c><00:13:33.779><c> forward</c><00:13:34.019><c> is</c><00:13:34.320><c> the</c><00:13:34.440><c> idea</c><00:13:34.680><c> that</c><00:13:35.040><c> in</c> drive people forward is the idea that in drive people forward is the idea that in order<00:13:35.639><c> to</c><00:13:36.060><c> immortalize</c><00:13:36.959><c> yourself</c><00:13:37.139><c> in</c><00:13:37.380><c> history</c> order to immortalize yourself in history order to immortalize yourself in history you<00:13:38.279><c> know</c><00:13:38.339><c> humans</c><00:13:39.000><c> have</c><00:13:39.180><c> to</c><00:13:39.300><c> survive</c><00:13:39.660><c> we</c><00:13:39.959><c> have</c> you know humans have to survive we have you know humans have to survive we have to<00:13:40.320><c> keep</c><00:13:40.920><c> moving</c><00:13:41.279><c> forward</c><00:13:41.519><c> and</c> to keep moving forward and to keep moving forward and um um um you<00:13:44.940><c> know</c><00:13:45.060><c> I</c><00:13:45.120><c> think</c><00:13:45.480><c> there's</c><00:13:45.600><c> a</c><00:13:45.720><c> lot</c><00:13:45.779><c> of</c> you know I think there's a lot of you know I think there's a lot of incentivization<00:13:46.500><c> to</c><00:13:47.040><c> uh</c> incentivization to uh incentivization to uh you<00:13:49.440><c> know</c><00:13:49.560><c> put</c><00:13:50.100><c> our</c><00:13:51.000><c> efforts</c><00:13:51.540><c> into</c> you know put our efforts into you know put our efforts into Cooperative<00:13:53.339><c> mechanisms</c> Cooperative mechanisms Cooperative mechanisms um<00:13:55.560><c> and</c><00:13:55.980><c> I</c><00:13:56.100><c> think</c><00:13:56.519><c> a</c><00:13:57.240><c> lot</c><00:13:57.360><c> of</c><00:13:57.540><c> these</c> um and I think a lot of these um and I think a lot of these conflicts<00:14:00.180><c> that</c><00:14:00.660><c> are</c><00:14:00.839><c> still</c><00:14:01.200><c> happening</c><00:14:01.500><c> in</c> conflicts that are still happening in conflicts that are still happening in the<00:14:01.740><c> world</c><00:14:01.800><c> are</c><00:14:02.279><c> eventually</c><00:14:02.639><c> going</c><00:14:03.060><c> to</c><00:14:04.100><c> die</c> the world are eventually going to die the world are eventually going to die out<00:14:05.339><c> in</c><00:14:05.940><c> favor</c><00:14:06.180><c> of</c><00:14:06.959><c> just</c><00:14:07.260><c> overall</c><00:14:07.680><c> progress</c> out in favor of just overall progress out in favor of just overall progress um<00:14:09.420><c> I</c><00:14:09.600><c> don't</c><00:14:09.899><c> know</c><00:14:10.019><c> it'll</c><00:14:10.320><c> be</c><00:14:10.440><c> entirely</c> um I don't know it'll be entirely um I don't know it'll be entirely uh uh uh you<00:14:13.560><c> know</c><00:14:13.620><c> peaceful</c><00:14:14.100><c> or</c><00:14:14.339><c> whatever</c><00:14:14.519><c> but</c><00:14:15.120><c> I</c> you know peaceful or whatever but I you know peaceful or whatever but I think<00:14:15.600><c> we</c><00:14:15.899><c> are</c><00:14:16.079><c> heading</c><00:14:16.560><c> that</c><00:14:16.680><c> way</c><00:14:16.860><c> and</c><00:14:17.100><c> that's</c> think we are heading that way and that's think we are heading that way and that's something<00:14:17.519><c> that</c><00:14:18.000><c> people</c><00:14:18.660><c> lose</c><00:14:18.959><c> sight</c><00:14:19.200><c> of</c><00:14:19.440><c> we</c> something that people lose sight of we something that people lose sight of we only<00:14:19.920><c> kind</c><00:14:20.279><c> of</c><00:14:20.399><c> have</c><00:14:20.459><c> lived</c><00:14:20.880><c> in</c><00:14:21.120><c> our</c><00:14:21.300><c> own</c> only kind of have lived in our own only kind of have lived in our own time time time uh<00:14:25.380><c> you</c><00:14:25.920><c> know</c><00:14:25.980><c> so</c><00:14:26.040><c> we</c><00:14:26.160><c> don't</c><00:14:26.279><c> understand</c> uh you know so we don't understand uh you know so we don't understand necessarily<00:14:27.060><c> the</c><00:14:27.360><c> the</c><00:14:27.540><c> violent</c><00:14:28.579><c> nature</c><00:14:29.579><c> of</c> necessarily the the violent nature of necessarily the the violent nature of people<00:14:30.300><c> in</c><00:14:30.540><c> the</c><00:14:30.660><c> past</c><00:14:30.779><c> and</c><00:14:31.079><c> how</c><00:14:31.260><c> we</c><00:14:31.440><c> are</c> people in the past and how we are people in the past and how we are changing<00:14:32.639><c> that</c> changing that changing that um<00:14:33.600><c> and</c><00:14:33.839><c> I</c><00:14:34.079><c> think</c><00:14:34.200><c> you</c><00:14:34.380><c> know</c><00:14:34.440><c> those</c><00:14:34.680><c> changes</c> um and I think you know those changes um and I think you know those changes have<00:14:35.279><c> come</c><00:14:35.459><c> about</c><00:14:35.579><c> very</c><00:14:36.060><c> very</c><00:14:36.180><c> rapidly</c><00:14:36.720><c> in</c><00:14:37.019><c> in</c> have come about very very rapidly in in have come about very very rapidly in in terms<00:14:37.440><c> of</c><00:14:37.500><c> human</c><00:14:37.740><c> history</c> terms of human history terms of human history um but<00:14:40.199><c> I</c><00:14:40.320><c> think</c><00:14:40.500><c> we</c><00:14:40.920><c> are</c> um but I think we are um but I think we are moving<00:14:42.660><c> toward</c><00:14:43.139><c> this</c><00:14:43.560><c> more</c><00:14:43.740><c> peaceful</c> moving toward this more peaceful moving toward this more peaceful existence<00:14:44.699><c> and</c> existence and existence and I<00:14:46.620><c> think</c><00:14:46.740><c> when</c><00:14:46.920><c> we</c><00:14:47.100><c> get</c><00:14:47.220><c> to</c><00:14:47.339><c> a</c><00:14:47.459><c> point</c><00:14:47.579><c> where</c> I think when we get to a point where I think when we get to a point where it's<00:14:48.860><c> post-scarcity</c><00:14:49.860><c> everybody</c><00:14:50.220><c> has</c><00:14:50.820><c> as</c><00:14:51.600><c> much</c> it's post-scarcity everybody has as much it's post-scarcity everybody has as much as<00:14:51.959><c> they</c><00:14:52.320><c> need</c><00:14:52.620><c> or</c><00:14:52.920><c> could</c><00:14:53.160><c> really</c><00:14:53.339><c> want</c> as they need or could really want as they need or could really want again<00:14:56.160><c> the</c><00:14:56.399><c> incentives</c><00:14:56.820><c> change</c><00:14:57.180><c> and</c> again the incentives change and again the incentives change and therefore<00:14:58.320><c> the</c><00:14:58.620><c> the</c><00:14:58.740><c> potential</c><00:14:59.160><c> outcomes</c> therefore the the potential outcomes therefore the the potential outcomes change<00:14:59.760><c> so</c><00:15:00.060><c> you</c><00:15:00.360><c> you</c><00:15:00.480><c> don't</c><00:15:00.720><c> see</c><00:15:00.959><c> a</c><00:15:01.199><c> lot</c><00:15:01.320><c> of</c> change so you you don't see a lot of change so you you don't see a lot of super<00:15:02.100><c> negative</c><00:15:02.579><c> outcomes</c><00:15:03.360><c> being</c><00:15:03.600><c> possible</c> super negative outcomes being possible super negative outcomes being possible but<00:15:06.000><c> we</c><00:15:06.120><c> do</c><00:15:06.240><c> have</c><00:15:06.360><c> to</c><00:15:06.420><c> make</c><00:15:06.540><c> it</c><00:15:06.660><c> to</c><00:15:06.779><c> that</c><00:15:06.959><c> point</c> but we do have to make it to that point but we do have to make it to that point um um um but<00:15:09.420><c> again</c><00:15:09.600><c> I</c><00:15:09.899><c> don't</c><00:15:09.959><c> think</c><00:15:10.139><c> AI</c><00:15:10.620><c> is</c><00:15:11.160><c> the</c> but again I don't think AI is the but again I don't think AI is the problem<00:15:11.519><c> there</c><00:15:11.760><c> I</c><00:15:11.940><c> still</c><00:15:12.120><c> think</c><00:15:12.300><c> it's</c><00:15:12.420><c> humans</c> problem there I still think it's humans problem there I still think it's humans so<00:15:13.860><c> in</c><00:15:14.040><c> a</c><00:15:14.160><c> way</c><00:15:14.279><c> that's</c><00:15:14.399><c> bad</c><00:15:14.839><c> that</c><00:15:15.839><c> that</c><00:15:15.959><c> I</c><00:15:16.139><c> think</c> so in a way that's bad that that I think so in a way that's bad that that I think is<00:15:16.440><c> quite</c><00:15:16.800><c> a</c><00:15:16.920><c> big</c><00:15:16.980><c> danger</c> is quite a big danger is quite a big danger we<00:15:18.420><c> are</c><00:15:18.600><c> all</c><00:15:19.079><c> of</c><00:15:19.199><c> us</c><00:15:19.320><c> already</c><00:15:19.680><c> are</c><00:15:20.220><c> cyborgs</c> we are all of us already are cyborgs we are all of us already are cyborgs um<00:15:22.620><c> so</c><00:15:23.220><c> you</c><00:15:23.579><c> have</c><00:15:23.760><c> a</c><00:15:24.120><c> machine</c><00:15:24.240><c> extension</c><00:15:24.779><c> of</c> um so you have a machine extension of um so you have a machine extension of yourself<00:15:25.139><c> in</c><00:15:25.680><c> the</c><00:15:25.860><c> form</c><00:15:25.980><c> of</c><00:15:26.160><c> your</c><00:15:26.399><c> your</c><00:15:26.699><c> phone</c> yourself in the form of your your phone yourself in the form of your your phone and<00:15:27.180><c> your</c><00:15:27.420><c> computer</c><00:15:27.600><c> and</c><00:15:28.380><c> all</c><00:15:28.500><c> your</c> and your computer and all your and your computer and all your applications<00:15:29.300><c> you</c><00:15:30.300><c> are</c><00:15:30.420><c> already</c><00:15:30.779><c> superhuman</c> applications you are already superhuman applications you are already superhuman well<00:15:32.760><c> by</c><00:15:33.060><c> far</c><00:15:33.240><c> you</c><00:15:33.540><c> have</c><00:15:33.720><c> more</c><00:15:33.899><c> more</c><00:15:34.260><c> powerful</c> well by far you have more more powerful well by far you have more more powerful capability<00:15:35.279><c> than</c><00:15:36.180><c> the</c><00:15:36.660><c> president</c><00:15:36.779><c> United</c> capability than the president United capability than the president United States<00:15:37.380><c> had</c><00:15:37.800><c> you</c><00:15:38.459><c> know</c><00:15:38.579><c> 30</c><00:15:38.760><c> years</c><00:15:38.940><c> ago</c> States had you know 30 years ago States had you know 30 years ago um this<00:15:40.680><c> is</c><00:15:40.800><c> true</c><00:15:40.920><c> if</c><00:15:41.100><c> you</c><00:15:41.220><c> have</c><00:15:41.279><c> an</c><00:15:41.399><c> internet</c> um this is true if you have an internet um this is true if you have an internet link<00:15:41.940><c> uh</c><00:15:42.839><c> you</c><00:15:43.019><c> you</c><00:15:43.199><c> have</c><00:15:43.380><c> an</c><00:15:43.500><c> article</c><00:15:43.860><c> of</c> link uh you you have an article of link uh you you have an article of wisdom<00:15:44.279><c> you</c><00:15:44.459><c> can</c><00:15:44.699><c> communicate</c><00:15:45.420><c> to</c><00:15:45.779><c> millions</c> wisdom you can communicate to millions wisdom you can communicate to millions of<00:15:46.320><c> people</c><00:15:46.440><c> who</c><00:15:46.560><c> can</c><00:15:46.740><c> communicate</c><00:15:47.100><c> to</c><00:15:47.220><c> the</c> of people who can communicate to the of people who can communicate to the rest<00:15:47.459><c> of</c><00:15:47.579><c> Earth</c><00:15:47.760><c> instantly</c> rest of Earth instantly rest of Earth instantly um I<00:15:50.100><c> mean</c><00:15:50.279><c> these</c><00:15:50.459><c> are</c><00:15:50.579><c> magical</c><00:15:50.880><c> powers</c><00:15:51.240><c> uh</c> um I mean these are magical powers uh um I mean these are magical powers uh that<00:15:53.339><c> didn't</c><00:15:53.459><c> exist</c><00:15:53.760><c> not</c><00:15:54.120><c> that</c><00:15:54.300><c> long</c><00:15:54.420><c> ago</c><00:15:54.720><c> so</c> that didn't exist not that long ago so that didn't exist not that long ago so everyone<00:15:55.860><c> is</c><00:15:56.279><c> already</c><00:15:56.519><c> superhuman</c> everyone is already superhuman everyone is already superhuman I<00:15:58.980><c> think</c><00:15:59.279><c> it's</c><00:15:59.459><c> the</c><00:15:59.820><c> singularity</c><00:16:00.420><c> is</c><00:16:00.600><c> probably</c> I think it's the singularity is probably I think it's the singularity is probably the<00:16:00.899><c> right</c><00:16:01.019><c> word</c><00:16:01.260><c> because</c><00:16:01.500><c> we</c><00:16:01.860><c> just</c><00:16:02.040><c> don't</c> the right word because we just don't the right word because we just don't know<00:16:02.459><c> what's</c><00:16:02.880><c> going</c><00:16:03.120><c> to</c><00:16:03.180><c> happen</c><00:16:03.360><c> once</c> know what's going to happen once know what's going to happen once uh<00:16:06.060><c> there's</c><00:16:06.480><c> intelligence</c><00:16:07.079><c> substantially</c> uh there's intelligence substantially uh there's intelligence substantially greater<00:16:08.519><c> than</c><00:16:08.639><c> that</c><00:16:08.880><c> of</c><00:16:09.120><c> a</c><00:16:09.240><c> human</c><00:16:09.480><c> brain</c> I<00:16:13.079><c> mean</c><00:16:13.320><c> most</c><00:16:13.500><c> of</c><00:16:13.620><c> the</c> I mean most of the I mean most of the movies<00:16:15.300><c> and</c><00:16:15.600><c> TV</c><00:16:15.899><c> featuring</c><00:16:16.440><c> AI</c><00:16:16.800><c> they</c><00:16:17.639><c> don't</c> movies and TV featuring AI they don't movies and TV featuring AI they don't describe<00:16:19.199><c> it</c><00:16:19.320><c> in</c><00:16:19.380><c> quite</c><00:16:19.620><c> the</c><00:16:19.800><c> way</c><00:16:19.980><c> it's</c><00:16:20.160><c> likely</c> describe it in quite the way it's likely describe it in quite the way it's likely to<00:16:20.760><c> actually</c><00:16:21.000><c> take</c><00:16:21.480><c> place</c><00:16:21.720><c> but</c><00:16:22.199><c> I</c><00:16:22.740><c> think</c><00:16:23.040><c> you</c> to actually take place but I think you to actually take place but I think you just<00:16:23.519><c> have</c><00:16:23.639><c> to</c><00:16:23.820><c> consider</c><00:16:24.180><c> like</c><00:16:24.600><c> even</c><00:16:24.839><c> in</c><00:16:25.139><c> the</c> just have to consider like even in the just have to consider like even in the benign<00:16:25.800><c> scenario</c><00:16:26.279><c> where</c> benign scenario where benign scenario where um<00:16:27.899><c> AI</c><00:16:28.440><c> if</c><00:16:28.800><c> AI</c><00:16:29.220><c> is</c><00:16:29.399><c> much</c><00:16:29.880><c> smarter</c><00:16:30.180><c> than</c><00:16:30.300><c> a</c> um AI if AI is much smarter than a um AI if AI is much smarter than a person person person um<00:16:32.100><c> what</c><00:16:32.339><c> what</c><00:16:32.399><c> do</c><00:16:32.699><c> we</c><00:16:32.880><c> do</c> um what what do we do um what what do we do what<00:16:34.500><c> what</c><00:16:34.560><c> is</c><00:16:34.920><c> that</c><00:16:35.040><c> what</c><00:16:35.279><c> job</c><00:16:35.459><c> do</c><00:16:35.699><c> we</c><00:16:35.820><c> have</c> what what is that what job do we have what what is that what job do we have I<00:16:37.139><c> have</c><00:16:37.320><c> to</c><00:16:37.500><c> say</c><00:16:37.620><c> that</c><00:16:37.740><c> when</c><00:16:37.860><c> you</c><00:16:38.040><c> know</c><00:16:38.160><c> when</c> I have to say that when you know when I have to say that when you know when okay<00:16:39.360><c> so</c><00:16:40.079><c> I've</c><00:16:40.320><c> actually</c><00:16:40.500><c> been</c><00:16:40.740><c> giving</c><00:16:40.980><c> this</c> okay so I've actually been giving this okay so I've actually been giving this specific<00:16:41.759><c> question</c><00:16:42.120><c> a</c><00:16:43.079><c> lot</c><00:16:43.320><c> of</c><00:16:43.500><c> thought</c> specific question a lot of thought specific question a lot of thought lately<00:16:44.040><c> and</c> lately and lately and you<00:16:46.380><c> know</c><00:16:46.440><c> I</c><00:16:46.860><c> think</c> you know I think you know I think because<00:16:49.500><c> of</c><00:16:49.740><c> the</c><00:16:49.980><c> way</c><00:16:50.100><c> we</c><00:16:50.459><c> frame</c><00:16:50.820><c> artificial</c> because of the way we frame artificial because of the way we frame artificial intelligence<00:16:51.959><c> and</c><00:16:52.680><c> this</c> intelligence and this intelligence and this idea<00:16:55.139><c> of</c><00:16:55.560><c> intelligence</c> idea of intelligence idea of intelligence we<00:16:57.839><c> kind</c><00:16:58.019><c> of</c><00:16:58.139><c> view</c><00:16:58.259><c> AI</c><00:16:58.680><c> as</c><00:16:58.920><c> being</c><00:16:59.100><c> the</c><00:16:59.279><c> one</c><00:16:59.399><c> that</c> we kind of view AI as being the one that we kind of view AI as being the one that does<00:16:59.699><c> the</c><00:16:59.940><c> thinking</c> does the thinking does the thinking um um um but<00:17:04.079><c> I</c><00:17:04.559><c> kind</c><00:17:04.679><c> of</c><00:17:04.799><c> look</c><00:17:05.100><c> at</c><00:17:05.280><c> it</c><00:17:05.459><c> a</c><00:17:05.699><c> much</c> but I kind of look at it a much but I kind of look at it a much different<00:17:06.000><c> way</c><00:17:06.380><c> and</c><00:17:07.380><c> so</c><00:17:07.679><c> you</c><00:17:07.919><c> know</c><00:17:08.040><c> typically</c> different way and so you know typically different way and so you know typically the<00:17:08.640><c> saying</c><00:17:09.059><c> has</c><00:17:09.240><c> been</c> the saying has been the saying has been execution<00:17:11.100><c> is</c><00:17:12.059><c> everything</c><00:17:12.299><c> ideas</c><00:17:12.720><c> are</c> execution is everything ideas are execution is everything ideas are nothing<00:17:13.260><c> right</c><00:17:13.880><c> but</c><00:17:14.880><c> in</c><00:17:15.299><c> my</c><00:17:15.419><c> eyes</c><00:17:15.600><c> AI</c><00:17:16.079><c> actually</c> nothing right but in my eyes AI actually nothing right but in my eyes AI actually gives<00:17:16.620><c> anybody</c><00:17:17.100><c> the</c><00:17:17.459><c> ability</c><00:17:17.699><c> to</c><00:17:18.059><c> execute</c> gives anybody the ability to execute gives anybody the ability to execute pretty<00:17:18.959><c> much</c><00:17:19.140><c> anything</c><00:17:19.520><c> and</c><00:17:20.520><c> so</c><00:17:20.819><c> with</c> pretty much anything and so with pretty much anything and so with execution<00:17:21.959><c> being</c><00:17:22.319><c> equal</c><00:17:22.640><c> it's</c><00:17:23.640><c> not</c><00:17:23.819><c> about</c> execution being equal it's not about execution being equal it's not about execution<00:17:24.600><c> it's</c><00:17:25.199><c> about</c><00:17:25.380><c> the</c><00:17:25.679><c> ideas</c><00:17:25.860><c> that</c> execution it's about the ideas that execution it's about the ideas that drive<00:17:26.459><c> that</c><00:17:26.760><c> execution</c><00:17:27.179><c> and</c><00:17:27.900><c> so</c><00:17:28.140><c> what</c><00:17:28.740><c> job</c><00:17:28.980><c> do</c> drive that execution and so what job do drive that execution and so what job do humans<00:17:29.520><c> have</c> humans have humans have we<00:17:31.140><c> steer</c><00:17:31.440><c> the</c><00:17:31.620><c> AIS</c><00:17:31.980><c> into</c><00:17:32.840><c> useful</c><00:17:33.840><c> and</c> we steer the AIS into useful and we steer the AIS into useful and productive<00:17:34.500><c> things</c><00:17:34.799><c> we</c><00:17:35.340><c> use</c><00:17:35.520><c> the</c><00:17:35.760><c> AIS</c><00:17:36.000><c> to</c><00:17:36.299><c> help</c> productive things we use the AIS to help productive things we use the AIS to help us<00:17:36.539><c> solve</c><00:17:36.840><c> the</c><00:17:36.900><c> problems</c><00:17:37.200><c> that</c><00:17:37.380><c> we</c><00:17:37.559><c> have</c><00:17:37.740><c> that</c> us solve the problems that we have that us solve the problems that we have that the<00:17:38.340><c> world</c><00:17:38.460><c> has</c> the world has the world has um and<00:17:40.980><c> we</c><00:17:41.100><c> use</c><00:17:41.280><c> these</c><00:17:41.520><c> in</c><00:17:41.700><c> a</c><00:17:42.000><c> Cooperative</c> um and we use these in a Cooperative um and we use these in a Cooperative mechanism mechanism mechanism um um um because<00:17:45.900><c> there's</c><00:17:46.620><c> way</c><00:17:47.100><c> too</c><00:17:47.220><c> much</c><00:17:47.400><c> abundance</c> because there's way too much abundance because there's way too much abundance to<00:17:48.000><c> be</c><00:17:48.120><c> had</c> to be had to be had with<00:17:49.559><c> with</c><00:17:49.559><c> that</c><00:17:49.980><c> being</c><00:17:50.160><c> the</c><00:17:50.340><c> case</c> with with that being the case with with that being the case and<00:17:52.080><c> so</c> and so and so we're<00:17:53.580><c> going</c><00:17:53.760><c> to</c><00:17:53.940><c> see</c><00:17:54.059><c> more</c><00:17:54.240><c> and</c><00:17:54.360><c> more</c><00:17:54.539><c> people</c> we're going to see more and more people we're going to see more and more people using<00:17:55.080><c> it</c><00:17:55.320><c> in</c><00:17:55.799><c> order</c><00:17:56.100><c> to</c> using it in order to using it in order to make<00:17:58.440><c> sure</c><00:17:58.559><c> that</c><00:17:58.919><c> not</c><00:17:59.160><c> only</c><00:17:59.340><c> themselves</c><00:17:59.760><c> are</c> make sure that not only themselves are make sure that not only themselves are financially<00:18:00.600><c> secure</c><00:18:00.960><c> but</c><00:18:01.260><c> their</c><00:18:01.440><c> communities</c> financially secure but their communities financially secure but their communities um um um you<00:18:04.260><c> know</c><00:18:04.320><c> their</c><00:18:04.559><c> friend</c><00:18:04.799><c> groups</c><00:18:05.280><c> it's</c><00:18:05.880><c> going</c> you know their friend groups it's going you know their friend groups it's going to<00:18:06.299><c> be</c><00:18:06.480><c> the</c><00:18:06.840><c> this</c><00:18:07.140><c> I</c><00:18:07.559><c> think</c><00:18:07.919><c> bottom-up</c><00:18:08.640><c> growth</c> to be the this I think bottom-up growth to be the this I think bottom-up growth of<00:18:09.179><c> wealth</c><00:18:09.600><c> whereas</c><00:18:10.380><c> it</c><00:18:10.559><c> used</c><00:18:10.679><c> to</c><00:18:10.860><c> be</c><00:18:10.980><c> kind</c><00:18:11.520><c> of</c> of wealth whereas it used to be kind of of wealth whereas it used to be kind of more<00:18:12.000><c> individual</c><00:18:12.660><c> in</c><00:18:13.080><c> nature</c> more individual in nature more individual in nature um um um the<00:18:16.320><c> people</c><00:18:16.500><c> who</c><00:18:16.740><c> realize</c><00:18:16.980><c> that</c><00:18:17.160><c> they</c><00:18:17.280><c> can</c><00:18:17.400><c> do</c> the people who realize that they can do the people who realize that they can do this<00:18:17.640><c> now</c><00:18:17.820><c> are</c><00:18:18.059><c> going</c><00:18:18.240><c> to</c><00:18:18.360><c> bring</c><00:18:18.480><c> groups</c><00:18:18.840><c> of</c> this now are going to bring groups of this now are going to bring groups of friends<00:18:19.080><c> together</c><00:18:19.380><c> to</c><00:18:20.160><c> collaborate</c><00:18:21.059><c> on</c> friends together to collaborate on friends together to collaborate on projects<00:18:21.840><c> to</c><00:18:22.380><c> work</c><00:18:22.679><c> together</c><00:18:22.919><c> people</c><00:18:23.760><c> are</c> projects to work together people are projects to work together people are going<00:18:24.120><c> to</c><00:18:24.299><c> work</c><00:18:24.419><c> with</c><00:18:24.600><c> people</c><00:18:24.840><c> that</c><00:18:25.020><c> they</c><00:18:25.200><c> want</c> going to work with people that they want going to work with people that they want to<00:18:25.440><c> work</c><00:18:25.559><c> with</c><00:18:25.799><c> instead</c><00:18:26.280><c> of</c><00:18:26.460><c> you</c><00:18:26.880><c> know</c><00:18:26.940><c> the</c> to work with instead of you know the to work with instead of you know the in<00:18:27.600><c> the</c><00:18:27.720><c> cubicle</c><00:18:28.020><c> next</c><00:18:28.200><c> door</c><00:18:28.380><c> just</c> in the cubicle next door just in the cubicle next door just because<00:18:28.740><c> that's</c><00:18:28.980><c> a</c><00:18:29.160><c> job</c><00:18:29.220><c> and</c><00:18:29.400><c> that</c><00:18:29.520><c> pays</c><00:18:29.820><c> the</c> because that's a job and that pays the because that's a job and that pays the bills<00:18:30.120><c> you</c><00:18:30.539><c> don't</c><00:18:30.720><c> have</c><00:18:30.840><c> to</c><00:18:30.900><c> do</c><00:18:30.960><c> that</c><00:18:31.080><c> you</c><00:18:31.260><c> have</c> bills you don't have to do that you have bills you don't have to do that you have a<00:18:31.559><c> choice</c> a choice a choice um um um so<00:18:34.020><c> ultimately</c><00:18:34.260><c> I</c><00:18:34.500><c> think</c><00:18:34.620><c> that's</c><00:18:34.740><c> what</c><00:18:34.919><c> we're</c> so ultimately I think that's what we're so ultimately I think that's what we're going<00:18:35.160><c> to</c><00:18:35.220><c> get</c><00:18:35.340><c> to</c><00:18:35.400><c> a</c><00:18:35.580><c> point</c><00:18:35.700><c> where</c><00:18:35.820><c> we</c><00:18:36.120><c> have</c><00:18:36.299><c> an</c> going to get to a point where we have an going to get to a point where we have an actual<00:18:37.020><c> choice</c><00:18:37.320><c> in</c><00:18:38.039><c> what</c><00:18:38.700><c> we're</c><00:18:38.820><c> doing</c><00:18:39.000><c> and</c> actual choice in what we're doing and actual choice in what we're doing and it's<00:18:39.480><c> going</c><00:18:39.600><c> to</c><00:18:39.660><c> be</c><00:18:39.780><c> about</c><00:18:39.840><c> the</c><00:18:40.080><c> ideas</c><00:18:40.260><c> that</c><00:18:40.860><c> we</c> it's going to be about the ideas that we it's going to be about the ideas that we use<00:18:41.100><c> to</c><00:18:41.280><c> drive</c><00:18:41.400><c> us</c><00:18:41.580><c> forward</c><00:18:41.760><c> something</c><00:18:42.240><c> is</c><00:18:42.480><c> the</c> use to drive us forward something is the use to drive us forward something is the danger<00:18:43.860><c> to</c><00:18:44.160><c> the</c><00:18:44.400><c> public</c><00:18:44.580><c> then</c><00:18:45.320><c> that</c><00:18:46.320><c> there</c> danger to the public then that there danger to the public then that there needs<00:18:47.039><c> to</c><00:18:47.100><c> be</c><00:18:47.220><c> some</c> needs to be some needs to be some government<00:18:48.720><c> agency</c><00:18:49.320><c> like</c><00:18:50.220><c> Regulators</c><00:18:51.120><c> the</c> government agency like Regulators the government agency like Regulators the fact<00:18:52.320><c> is</c><00:18:52.440><c> like</c><00:18:52.679><c> we've</c><00:18:52.860><c> got</c><00:18:52.980><c> Regulators</c><00:18:53.520><c> in</c> fact is like we've got Regulators in fact is like we've got Regulators in um you<00:18:55.500><c> know</c><00:18:55.620><c> the</c><00:18:55.740><c> aircraft</c><00:18:55.980><c> industry</c><00:18:56.520><c> car</c> um you know the aircraft industry car um you know the aircraft industry car industry<00:18:57.600><c> uh</c><00:18:58.500><c> drugs</c><00:18:59.340><c> food</c> industry uh drugs food industry uh drugs food um you<00:19:01.380><c> know</c><00:19:01.559><c> and</c><00:19:01.799><c> anything</c><00:19:02.280><c> that's</c><00:19:02.460><c> sort</c><00:19:02.640><c> of</c> um you know and anything that's sort of um you know and anything that's sort of a<00:19:02.820><c> public</c><00:19:02.940><c> risk</c><00:19:03.480><c> I</c><00:19:04.140><c> mean</c><00:19:04.260><c> I</c><00:19:04.320><c> think</c><00:19:04.500><c> this</c><00:19:04.799><c> has</c><00:19:05.340><c> to</c> a public risk I mean I think this has to a public risk I mean I think this has to pull<00:19:05.640><c> into</c><00:19:05.820><c> the</c><00:19:06.059><c> category</c><00:19:06.419><c> of</c><00:19:06.539><c> a</c><00:19:06.720><c> public</c><00:19:06.840><c> risk</c> pull into the category of a public risk pull into the category of a public risk usually<00:19:07.860><c> it'll</c><00:19:08.100><c> be</c><00:19:08.220><c> something</c><00:19:08.400><c> some</c><00:19:08.760><c> new</c> usually it'll be something some new usually it'll be something some new technology<00:19:09.360><c> which</c><00:19:10.080><c> will</c><00:19:10.380><c> cause</c><00:19:10.620><c> damage</c><00:19:11.520><c> or</c> technology which will cause damage or technology which will cause damage or death<00:19:11.880><c> it</c><00:19:12.480><c> will</c><00:19:12.660><c> be</c><00:19:12.780><c> an</c><00:19:12.900><c> outcry</c><00:19:13.260><c> they</c><00:19:14.100><c> will</c><00:19:14.220><c> be</c> death it will be an outcry they will be death it will be an outcry they will be an<00:19:14.520><c> investigation</c><00:19:15.000><c> years</c><00:19:15.840><c> will</c><00:19:16.140><c> pass</c> an investigation years will pass an investigation years will pass there<00:19:17.520><c> will</c><00:19:17.760><c> be</c> there will be there will be some<00:19:19.260><c> sort</c><00:19:19.380><c> of</c><00:19:19.500><c> insight</c><00:19:19.860><c> committee</c><00:19:20.400><c> they</c><00:19:21.000><c> will</c> some sort of insight committee they will some sort of insight committee they will be<00:19:21.299><c> rule</c><00:19:21.600><c> making</c><00:19:21.780><c> then</c><00:19:22.500><c> there</c><00:19:22.740><c> will</c><00:19:22.919><c> be</c> be rule making then there will be be rule making then there will be oversight<00:19:23.520><c> eventually</c><00:19:24.240><c> regulations</c> oversight eventually regulations oversight eventually regulations this<00:19:26.460><c> all</c><00:19:26.640><c> takes</c><00:19:26.940><c> many</c><00:19:27.179><c> years</c> this all takes many years this all takes many years this<00:19:28.559><c> is</c><00:19:28.679><c> the</c><00:19:28.860><c> normal</c><00:19:28.980><c> course</c><00:19:29.280><c> of</c><00:19:29.520><c> things</c><00:19:29.700><c> if</c> this is the normal course of things if this is the normal course of things if you<00:19:30.179><c> look</c><00:19:30.299><c> at</c><00:19:30.419><c> say</c> you look at say you look at say Automotive<00:19:32.280><c> regulations</c><00:19:32.700><c> how</c><00:19:33.059><c> long</c><00:19:33.179><c> did</c><00:19:33.360><c> it</c> Automotive regulations how long did it Automotive regulations how long did it take<00:19:33.660><c> for</c><00:19:33.840><c> seat</c><00:19:34.140><c> belts</c><00:19:34.559><c> to</c><00:19:35.160><c> be</c><00:19:35.340><c> implemented</c><00:19:35.880><c> to</c> take for seat belts to be implemented to take for seat belts to be implemented to be<00:19:36.179><c> required</c> be required be required you<00:19:37.799><c> know</c><00:19:37.860><c> the</c><00:19:37.980><c> order</c><00:19:38.100><c> industry</c><00:19:38.520><c> Fort</c><00:19:38.700><c> seat</c> you know the order industry Fort seat you know the order industry Fort seat belts<00:19:39.539><c> I</c><00:19:39.660><c> think</c><00:19:39.780><c> for</c><00:19:40.080><c> more</c><00:19:40.500><c> than</c><00:19:40.679><c> a</c><00:19:40.799><c> decade</c> belts I think for more than a decade belts I think for more than a decade successfully<00:19:42.660><c> afford</c> successfully afford successfully afford any<00:19:44.100><c> regulations</c><00:19:44.580><c> on</c><00:19:45.179><c> seat</c><00:19:45.480><c> belts</c><00:19:45.840><c> even</c> any regulations on seat belts even any regulations on seat belts even though though though the<00:19:48.000><c> numbers</c><00:19:48.360><c> were</c><00:19:48.720><c> extremely</c><00:19:49.260><c> obvious</c><00:19:49.679><c> if</c> the numbers were extremely obvious if the numbers were extremely obvious if you<00:19:50.400><c> had</c><00:19:50.580><c> a</c><00:19:50.700><c> seat</c><00:19:50.820><c> belt</c><00:19:51.240><c> on</c> you had a seat belt on you had a seat belt on you<00:19:53.280><c> would</c><00:19:53.460><c> be</c><00:19:53.640><c> far</c><00:19:54.059><c> less</c><00:19:54.240><c> likely</c><00:19:54.539><c> to</c><00:19:54.660><c> die</c><00:19:54.840><c> or</c> you would be far less likely to die or you would be far less likely to die or be<00:19:55.200><c> seriously</c><00:19:55.440><c> injured</c><00:19:55.799><c> it</c><00:19:55.980><c> was</c><00:19:56.100><c> unequivocal</c> be seriously injured it was unequivocal be seriously injured it was unequivocal and<00:19:57.539><c> the</c><00:19:57.660><c> industry</c><00:19:57.960><c> fought</c><00:19:58.260><c> this</c><00:19:59.039><c> for</c><00:19:59.460><c> years</c> and the industry fought this for years and the industry fought this for years successfully<00:20:00.679><c> eventually</c><00:20:01.679><c> after</c><00:20:02.660><c> many</c><00:20:03.660><c> many</c> successfully eventually after many many successfully eventually after many many people<00:20:04.200><c> died</c> people died people died Regulators<00:20:06.179><c> insisted</c><00:20:06.720><c> on</c><00:20:06.960><c> seat</c><00:20:07.260><c> belts</c> Regulators insisted on seat belts Regulators insisted on seat belts this<00:20:09.419><c> is</c><00:20:09.539><c> a</c><00:20:09.840><c> if</c><00:20:10.320><c> this</c><00:20:10.500><c> time</c><00:20:10.679><c> frame</c><00:20:10.919><c> is</c><00:20:11.039><c> not</c> this is a if this time frame is not this is a if this time frame is not relevant<00:20:11.580><c> to</c><00:20:11.760><c> AI</c> relevant to AI relevant to AI okay<00:20:13.340><c> so</c><00:20:14.340><c> a</c><00:20:14.820><c> lot</c><00:20:14.940><c> to</c><00:20:15.179><c> touch</c><00:20:15.360><c> on</c><00:20:15.539><c> in</c><00:20:16.320><c> this</c><00:20:16.740><c> bit</c><00:20:16.860><c> of</c> okay so a lot to touch on in this bit of okay so a lot to touch on in this bit of it it it um um um first<00:20:20.039><c> of</c><00:20:20.220><c> all</c><00:20:20.340><c> I</c><00:20:20.820><c> think</c><00:20:20.960><c> the</c><00:20:21.960><c> idea</c><00:20:22.200><c> of</c> first of all I think the idea of first of all I think the idea of Regulation<00:20:22.940><c> uh</c><00:20:23.940><c> is</c><00:20:24.059><c> is</c><00:20:24.240><c> the</c><00:20:24.419><c> double-edged</c> Regulation uh is is the double-edged Regulation uh is is the double-edged sword<00:20:25.280><c> and</c><00:20:26.280><c> so</c><00:20:26.940><c> by</c><00:20:27.240><c> that</c><00:20:27.419><c> I</c><00:20:27.720><c> mean</c><00:20:27.840><c> we</c><00:20:28.799><c> have</c><00:20:28.919><c> a</c> sword and so by that I mean we have a sword and so by that I mean we have a lot<00:20:29.160><c> of</c><00:20:29.280><c> regulations</c><00:20:29.640><c> on</c><00:20:29.880><c> things</c><00:20:30.120><c> yes</c> lot of regulations on things yes lot of regulations on things yes um<00:20:32.400><c> for</c><00:20:32.820><c> the</c><00:20:32.940><c> majority</c><00:20:33.240><c> of</c><00:20:33.419><c> them</c><00:20:33.720><c> I</c><00:20:33.960><c> think</c><00:20:34.260><c> we</c> um for the majority of them I think we um for the majority of them I think we are<00:20:34.860><c> good</c><00:20:35.360><c> uh</c><00:20:36.360><c> however</c> are good uh however are good uh however even<00:20:38.820><c> when</c><00:20:39.120><c> you</c><00:20:39.240><c> look</c><00:20:39.480><c> at</c><00:20:40.080><c> like</c><00:20:40.860><c> the</c><00:20:41.039><c> FDA</c><00:20:41.460><c> Food</c> even when you look at like the FDA Food even when you look at like the FDA Food and<00:20:41.940><c> Drug</c><00:20:42.059><c> Administration</c><00:20:42.600><c> in</c><00:20:43.320><c> terms</c><00:20:43.679><c> of</c><00:20:43.919><c> what</c> and Drug Administration in terms of what and Drug Administration in terms of what they<00:20:44.340><c> regulate</c> they regulate they regulate there's<00:20:46.919><c> already</c><00:20:47.220><c> too</c><00:20:47.580><c> much</c><00:20:47.760><c> power</c><00:20:48.179><c> in</c><00:20:48.660><c> the</c> there's already too much power in the there's already too much power in the ability<00:20:49.080><c> of</c><00:20:49.320><c> people</c><00:20:49.500><c> to</c><00:20:49.799><c> fight</c><00:20:50.039><c> that</c> ability of people to fight that ability of people to fight that regulation<00:20:50.820><c> or</c><00:20:51.240><c> make</c><00:20:51.419><c> very</c><00:20:51.720><c> specific</c><00:20:52.400><c> carve</c> regulation or make very specific carve regulation or make very specific carve outs<00:20:53.760><c> for</c><00:20:54.179><c> their</c><00:20:54.360><c> own</c> outs for their own outs for their own um<00:20:55.799><c> interests</c> um interests um interests um oddly<00:21:01.440><c> enough</c><00:21:01.620><c> and</c><00:21:02.460><c> this</c><00:21:03.000><c> is</c> oddly enough and this is oddly enough and this is me<00:21:04.140><c> going</c><00:21:04.380><c> off</c><00:21:04.559><c> the</c><00:21:04.679><c> rails</c><00:21:04.919><c> a</c><00:21:05.039><c> bit</c><00:21:05.100><c> here</c><00:21:05.280><c> but</c> me going off the rails a bit here but me going off the rails a bit here but uh uh uh I<00:21:09.059><c> don't</c><00:21:09.179><c> know</c><00:21:09.299><c> that</c><00:21:09.480><c> human</c><00:21:09.720><c> regulation</c><00:21:10.260><c> is</c> I don't know that human regulation is I don't know that human regulation is the<00:21:10.679><c> key</c> the key the key um um um I<00:21:13.860><c> I</c><00:21:13.860><c> almost</c><00:21:14.340><c> think</c><00:21:14.700><c> that</c><00:21:15.140><c> AI</c><00:21:16.140><c> could</c> I I almost think that AI could I I almost think that AI could potentially<00:21:16.740><c> help</c><00:21:16.919><c> us</c><00:21:17.039><c> regulate</c><00:21:17.520><c> AI</c><00:21:17.820><c> much</c> potentially help us regulate AI much potentially help us regulate AI much better<00:21:18.240><c> than</c><00:21:18.419><c> humans</c><00:21:18.780><c> in</c><00:21:19.440><c> the</c><00:21:19.559><c> sense</c><00:21:19.679><c> that</c><00:21:20.160><c> we</c> better than humans in the sense that we better than humans in the sense that we already<00:21:21.059><c> have</c> already have already have government<00:21:22.940><c> officials</c><00:21:24.320><c> who</c><00:21:25.320><c> are</c><00:21:25.980><c> trying</c><00:21:26.520><c> to</c> government officials who are trying to government officials who are trying to pass<00:21:26.880><c> laws</c><00:21:27.240><c> about</c><00:21:27.299><c> things</c><00:21:27.539><c> they</c><00:21:27.720><c> don't</c> pass laws about things they don't pass laws about things they don't understand<00:21:28.020><c> and</c><00:21:28.320><c> they</c><00:21:28.500><c> have</c><00:21:28.559><c> no</c><00:21:28.679><c> desire</c><00:21:28.980><c> to</c> understand and they have no desire to understand and they have no desire to understand understand understand and and and you<00:21:31.980><c> see</c><00:21:32.159><c> this</c><00:21:32.460><c> with</c><00:21:33.179><c> some</c><00:21:33.600><c> of</c><00:21:33.720><c> the</c><00:21:33.780><c> stuff</c> you see this with some of the stuff you see this with some of the stuff that's<00:21:34.080><c> happening</c><00:21:34.500><c> with</c><00:21:34.980><c> social</c><00:21:35.280><c> media</c><00:21:35.780><c> uh</c> that's happening with social media uh that's happening with social media uh and<00:21:36.900><c> all</c><00:21:37.200><c> of</c><00:21:37.380><c> that</c><00:21:37.500><c> and</c><00:21:37.980><c> the</c><00:21:38.159><c> thing</c><00:21:38.340><c> that</c><00:21:38.460><c> I</c> and all of that and the thing that I and all of that and the thing that I think<00:21:38.640><c> is</c><00:21:38.760><c> kind</c><00:21:38.940><c> of</c><00:21:39.000><c> funny</c><00:21:39.120><c> is</c><00:21:39.780><c> you</c><00:21:39.900><c> know</c><00:21:40.020><c> Elon</c> think is kind of funny is you know Elon think is kind of funny is you know Elon is<00:21:40.559><c> talking</c><00:21:40.799><c> about</c><00:21:41.039><c> you</c><00:21:41.880><c> know</c><00:21:42.000><c> putting</c> is talking about you know putting is talking about you know putting regulations<00:21:43.320><c> in</c><00:21:43.740><c> place</c> regulations in place regulations in place um um um but<00:21:46.320><c> not</c><00:21:46.440><c> at</c><00:21:46.620><c> Twitter</c><00:21:46.799><c> we</c><00:21:47.100><c> don't</c><00:21:47.220><c> want</c> but not at Twitter we don't want but not at Twitter we don't want regulations<00:21:47.820><c> to</c><00:21:48.000><c> Twitter</c><00:21:48.179><c> we</c><00:21:48.360><c> don't</c><00:21:48.419><c> want</c><00:21:48.539><c> any</c> regulations to Twitter we don't want any regulations to Twitter we don't want any government<00:21:48.960><c> oversight</c><00:21:49.440><c> there</c> government oversight there government oversight there um<00:21:50.640><c> and</c><00:21:51.360><c> I'm</c><00:21:51.539><c> not</c><00:21:51.720><c> necessarily</c><00:21:52.200><c> saying</c><00:21:52.559><c> that</c> um and I'm not necessarily saying that um and I'm not necessarily saying that there<00:21:52.980><c> should</c><00:21:53.400><c> be</c><00:21:53.520><c> government</c><00:21:54.299><c> regulation</c><00:21:54.960><c> of</c> there should be government regulation of there should be government regulation of platforms<00:21:56.159><c> like</c><00:21:56.280><c> Twitter</c> platforms like Twitter platforms like Twitter but<00:21:57.659><c> there's</c><00:21:57.840><c> also</c><00:21:58.020><c> a</c><00:21:58.140><c> lot</c><00:21:58.200><c> of</c><00:21:58.320><c> risk</c><00:21:58.559><c> there</c><00:21:58.740><c> you</c> but there's also a lot of risk there you but there's also a lot of risk there you we<00:21:59.520><c> we've</c><00:21:59.880><c> seen</c><00:22:00.000><c> what</c><00:22:00.179><c> can</c><00:22:00.360><c> happen</c> we we've seen what can happen we we've seen what can happen um<00:22:01.980><c> with</c><00:22:02.640><c> the</c><00:22:02.760><c> ability</c><00:22:02.940><c> to</c><00:22:03.240><c> communicate</c><00:22:03.600><c> to</c> um with the ability to communicate to um with the ability to communicate to large<00:22:04.020><c> numbers</c><00:22:04.260><c> of</c><00:22:04.500><c> people</c><00:22:04.679><c> and</c><00:22:04.919><c> the</c><00:22:05.100><c> fact</c> large numbers of people and the fact large numbers of people and the fact that<00:22:05.340><c> anybody</c><00:22:05.640><c> can</c><00:22:05.820><c> do</c><00:22:06.000><c> that</c><00:22:06.200><c> this</c> that anybody can do that this that anybody can do that this algorithmic<00:22:07.980><c> boosting</c><00:22:08.460><c> this</c><00:22:09.360><c> AI</c><00:22:09.840><c> that</c><00:22:10.320><c> boosts</c> algorithmic boosting this AI that boosts algorithmic boosting this AI that boosts posts<00:22:11.340><c> and</c><00:22:12.120><c> spreads</c><00:22:12.780><c> information</c> posts and spreads information posts and spreads information um<00:22:15.480><c> can</c><00:22:16.080><c> have</c> um can have um can have large<00:22:18.419><c> effects</c><00:22:18.900><c> on</c><00:22:19.380><c> the</c><00:22:19.740><c> world</c><00:22:19.919><c> and</c><00:22:20.580><c> so</c> large effects on the world and so large effects on the world and so you<00:22:22.260><c> know</c><00:22:22.320><c> it</c><00:22:22.620><c> is</c><00:22:23.039><c> kind</c><00:22:23.220><c> of</c><00:22:23.340><c> funny</c><00:22:23.520><c> that</c><00:22:23.880><c> he's</c> you know it is kind of funny that he's you know it is kind of funny that he's arguing<00:22:24.480><c> for</c><00:22:24.659><c> regulation</c><00:22:25.140><c> in</c><00:22:25.380><c> some</c><00:22:25.559><c> aspects</c> arguing for regulation in some aspects arguing for regulation in some aspects of<00:22:26.039><c> AI</c><00:22:26.340><c> but</c><00:22:26.700><c> like</c><00:22:26.880><c> not</c><00:22:27.059><c> at</c><00:22:27.240><c> his</c><00:22:27.419><c> companies</c> of AI but like not at his companies of AI but like not at his companies um<00:22:29.220><c> and</c><00:22:29.900><c> that's</c><00:22:30.900><c> probably</c><00:22:31.080><c> a</c><00:22:31.260><c> broad</c> um and that's probably a broad um and that's probably a broad overstatement<00:22:32.100><c> you</c><00:22:32.520><c> know</c><00:22:32.640><c> I</c><00:22:32.760><c> think</c><00:22:32.880><c> he</c> overstatement you know I think he overstatement you know I think he welcomes<00:22:33.480><c> some</c><00:22:33.659><c> of</c><00:22:33.840><c> the</c><00:22:33.980><c> uh</c><00:22:34.980><c> regulations</c><00:22:35.580><c> in</c> welcomes some of the uh regulations in welcomes some of the uh regulations in terms<00:22:36.299><c> of</c><00:22:36.659><c> like</c><00:22:37.080><c> self-driving</c><00:22:37.620><c> Ai</c><00:22:38.039><c> and</c><00:22:38.159><c> things</c> terms of like self-driving Ai and things terms of like self-driving Ai and things like<00:22:38.580><c> that</c><00:22:38.840><c> but</c><00:22:39.840><c> he's</c><00:22:39.960><c> been</c><00:22:40.140><c> very</c><00:22:40.260><c> very</c> like that but he's been very very like that but he's been very very outspoken<00:22:41.039><c> about</c><00:22:41.159><c> how</c><00:22:41.460><c> much</c><00:22:41.520><c> he</c><00:22:41.760><c> doesn't</c><00:22:41.940><c> want</c> outspoken about how much he doesn't want outspoken about how much he doesn't want the<00:22:43.200><c> government</c><00:22:43.440><c> involvement</c><00:22:43.980><c> in</c><00:22:44.340><c> Twitter</c> the government involvement in Twitter the government involvement in Twitter um<00:22:46.140><c> sorry</c><00:22:46.679><c> X</c><00:22:47.100><c> whatever</c><00:22:47.820><c> but</c> um sorry X whatever but um sorry X whatever but you<00:22:51.000><c> know</c><00:22:51.120><c> I</c><00:22:51.360><c> think</c> you know I think you know I think bad<00:22:53.520><c> regulation</c> bad regulation bad regulation can<00:22:55.500><c> be</c><00:22:55.679><c> actually</c><00:22:55.980><c> more</c><00:22:56.640><c> harmful</c><00:22:57.059><c> than</c><00:22:57.720><c> no</c> can be actually more harmful than no can be actually more harmful than no regulation<00:22:58.559><c> because</c><00:22:58.860><c> we</c><00:22:59.100><c> were</c><00:22:59.220><c> talking</c><00:22:59.400><c> about</c> regulation because we were talking about regulation because we were talking about the<00:22:59.880><c> fact</c><00:23:00.000><c> that</c><00:23:00.179><c> we</c><00:23:00.419><c> need</c><00:23:00.539><c> to</c><00:23:00.659><c> have</c><00:23:00.840><c> these</c> the fact that we need to have these the fact that we need to have these balanced<00:23:01.940><c> uh</c><00:23:02.940><c> offerings</c><00:23:03.299><c> this</c> balanced uh offerings this balanced uh offerings this democratization<00:23:04.200><c> of</c><00:23:04.500><c> access</c><00:23:04.679><c> to</c><00:23:04.980><c> Ai</c><00:23:05.480><c> and</c><00:23:06.480><c> what</c> democratization of access to Ai and what democratization of access to Ai and what regulation<00:23:07.200><c> is</c><00:23:07.440><c> going</c><00:23:07.559><c> to</c><00:23:07.679><c> end</c><00:23:07.799><c> up</c><00:23:07.919><c> doing</c><00:23:08.159><c> is</c> regulation is going to end up doing is regulation is going to end up doing is it's<00:23:08.580><c> going</c><00:23:08.760><c> to</c><00:23:08.760><c> create</c><00:23:08.880><c> this</c><00:23:09.120><c> regulatory</c> it's going to create this regulatory it's going to create this regulatory capture<00:23:10.140><c> which</c><00:23:10.679><c> ensures</c><00:23:11.039><c> that</c><00:23:11.280><c> only</c><00:23:11.520><c> the</c> capture which ensures that only the capture which ensures that only the select<00:23:12.120><c> few</c><00:23:12.299><c> have</c><00:23:12.720><c> access</c><00:23:12.960><c> to</c><00:23:13.260><c> regulations</c> select few have access to regulations select few have access to regulations because<00:23:13.919><c> they're</c><00:23:14.100><c> the</c><00:23:14.280><c> only</c><00:23:14.400><c> ones</c><00:23:14.640><c> with</c> because they're the only ones with because they're the only ones with enough<00:23:14.940><c> money</c><00:23:15.240><c> to</c><00:23:15.539><c> ensure</c><00:23:15.960><c> that</c><00:23:16.559><c> they</c><00:23:16.679><c> hire</c> enough money to ensure that they hire enough money to ensure that they hire the<00:23:17.100><c> right</c><00:23:17.220><c> people</c><00:23:17.460><c> to</c><00:23:17.700><c> make</c><00:23:17.820><c> sure</c><00:23:17.940><c> that</c><00:23:18.240><c> you</c> the right people to make sure that you the right people to make sure that you know<00:23:18.539><c> the</c><00:23:19.020><c> t's</c><00:23:19.320><c> are</c><00:23:19.440><c> crowded</c><00:23:19.679><c> the</c><00:23:19.799><c> eyes</c><00:23:19.919><c> are</c> know the t's are crowded the eyes are know the t's are crowded the eyes are DOT<00:23:20.460><c> or</c><00:23:20.820><c> the</c><00:23:21.000><c> t's</c><00:23:21.419><c> across</c><00:23:21.780><c> the</c><00:23:21.840><c> eyes</c><00:23:21.960><c> are</c> DOT or the t's across the eyes are DOT or the t's across the eyes are dotted dotted dotted um um um and<00:23:25.020><c> their</c><00:23:25.440><c> AI</c><00:23:25.860><c> is</c><00:23:26.640><c> in</c><00:23:27.000><c> favor</c><00:23:27.179><c> of</c><00:23:27.840><c> whoever's</c><00:23:28.380><c> in</c> and their AI is in favor of whoever's in and their AI is in favor of whoever's in government<00:23:29.000><c> and</c><00:23:30.000><c> so</c><00:23:30.360><c> they're</c><00:23:30.480><c> going</c><00:23:30.659><c> to</c><00:23:30.720><c> have</c> government and so they're going to have government and so they're going to have access<00:23:31.020><c> to</c><00:23:31.320><c> that</c><00:23:31.440><c> and</c><00:23:31.620><c> it's</c><00:23:31.799><c> going</c><00:23:31.980><c> to</c><00:23:32.159><c> price</c> access to that and it's going to price access to that and it's going to price out<00:23:32.760><c> the</c><00:23:33.659><c> average</c> out the average out the average business<00:23:35.700><c> from</c><00:23:36.179><c> being</c><00:23:36.419><c> able</c><00:23:36.659><c> to</c><00:23:36.780><c> step</c><00:23:36.900><c> into</c> business from being able to step into business from being able to step into that<00:23:37.320><c> and</c><00:23:37.440><c> you</c><00:23:37.559><c> already</c><00:23:37.799><c> see</c><00:23:37.980><c> this</c><00:23:38.100><c> with</c> that and you already see this with that and you already see this with highly<00:23:38.520><c> regulated</c><00:23:38.940><c> Industries</c> highly regulated Industries highly regulated Industries um<00:23:40.380><c> you</c><00:23:40.500><c> know</c><00:23:40.620><c> you</c><00:23:40.679><c> look</c><00:23:40.860><c> at</c><00:23:40.919><c> Healthcare</c><00:23:41.220><c> you</c> um you know you look at Healthcare you um you know you look at Healthcare you look<00:23:41.520><c> at</c><00:23:41.640><c> Education</c><00:23:42.059><c> costs</c><00:23:42.960><c> are</c><00:23:43.080><c> skyrocketing</c> look at Education costs are skyrocketing look at Education costs are skyrocketing um<00:23:45.059><c> and</c><00:23:45.360><c> if</c><00:23:45.539><c> you</c><00:23:45.659><c> listen</c><00:23:45.720><c> to</c><00:23:45.840><c> anything</c><00:23:46.020><c> like</c> um and if you listen to anything like um and if you listen to anything like Mark<00:23:46.380><c> Andreessen</c><00:23:47.039><c> says</c><00:23:47.159><c> he</c><00:23:47.280><c> talks</c><00:23:47.520><c> a</c><00:23:47.640><c> lot</c> Mark Andreessen says he talks a lot Mark Andreessen says he talks a lot about<00:23:47.760><c> that</c> about that about that um<00:23:49.260><c> and</c><00:23:49.919><c> you</c><00:23:50.220><c> know</c><00:23:50.340><c> I</c><00:23:50.400><c> don't</c><00:23:50.460><c> agree</c><00:23:50.640><c> with</c> um and you know I don't agree with um and you know I don't agree with everything<00:23:50.940><c> that</c><00:23:51.179><c> Mark</c><00:23:51.360><c> says</c><00:23:51.659><c> but</c><00:23:51.840><c> I</c><00:23:51.960><c> think</c><00:23:52.020><c> he</c> everything that Mark says but I think he everything that Mark says but I think he has<00:23:52.320><c> a</c><00:23:52.440><c> lot</c><00:23:52.500><c> of</c><00:23:52.679><c> uh</c><00:23:53.100><c> you</c><00:23:53.280><c> know</c><00:23:53.340><c> really</c><00:23:53.520><c> good</c> has a lot of uh you know really good has a lot of uh you know really good observations<00:23:54.179><c> on</c><00:23:54.960><c> these</c><00:23:55.440><c> things</c><00:23:55.559><c> that</c><00:23:55.740><c> happen</c> observations on these things that happen observations on these things that happen uh<00:23:57.059><c> but</c><00:23:57.240><c> when</c><00:23:57.419><c> we</c><00:23:57.539><c> do</c><00:23:57.720><c> highly</c><00:23:58.140><c> regulate</c><00:23:58.559><c> things</c> uh but when we do highly regulate things uh but when we do highly regulate things because<00:24:00.360><c> of</c><00:24:00.539><c> the</c><00:24:00.600><c> way</c><00:24:00.720><c> our</c><00:24:00.900><c> government</c><00:24:01.080><c> is</c><00:24:01.320><c> set</c> because of the way our government is set because of the way our government is set up<00:24:01.620><c> the</c><00:24:01.919><c> people</c><00:24:02.100><c> with</c><00:24:02.340><c> money</c><00:24:02.580><c> and</c><00:24:02.760><c> the</c> up the people with money and the up the people with money and the resources<00:24:03.299><c> are</c><00:24:03.480><c> the</c><00:24:03.659><c> ones</c><00:24:03.840><c> who</c><00:24:04.140><c> can</c><00:24:04.320><c> you</c><00:24:05.159><c> know</c> resources are the ones who can you know resources are the ones who can you know use<00:24:06.059><c> regulatory</c><00:24:06.659><c> capture</c><00:24:07.080><c> to</c><00:24:07.440><c> actually</c> use regulatory capture to actually use regulatory capture to actually gain<00:24:09.059><c> a</c><00:24:09.240><c> foothold</c><00:24:09.480><c> in</c><00:24:09.600><c> that</c><00:24:09.780><c> market</c><00:24:09.960><c> and</c><00:24:10.740><c> so</c><00:24:10.980><c> I</c> gain a foothold in that market and so I gain a foothold in that market and so I do<00:24:11.400><c> think</c><00:24:11.640><c> that</c><00:24:11.880><c> if</c><00:24:12.120><c> we</c><00:24:12.240><c> want</c><00:24:12.360><c> to</c><00:24:12.480><c> do</c><00:24:12.600><c> any</c><00:24:12.840><c> sort</c> do think that if we want to do any sort do think that if we want to do any sort of<00:24:13.020><c> Regulation</c><00:24:13.440><c> around</c><00:24:13.679><c> AI</c><00:24:14.100><c> we</c><00:24:14.760><c> have</c><00:24:14.820><c> to</c><00:24:14.940><c> very</c> of Regulation around AI we have to very of Regulation around AI we have to very carefully<00:24:15.539><c> consider</c><00:24:16.200><c> how</c><00:24:16.919><c> that</c><00:24:17.039><c> regulation</c> carefully consider how that regulation carefully consider how that regulation is<00:24:17.640><c> going</c><00:24:17.820><c> to</c><00:24:17.880><c> be</c><00:24:17.940><c> enacted</c><00:24:18.360><c> because</c><00:24:18.659><c> let's</c> is going to be enacted because let's is going to be enacted because let's face<00:24:19.860><c> it</c><00:24:20.159><c> we're</c><00:24:20.580><c> not</c><00:24:20.760><c> the</c><00:24:20.940><c> best</c><00:24:21.059><c> at</c><00:24:21.240><c> it</c><00:24:21.360><c> and</c> face it we're not the best at it and face it we're not the best at it and what<00:24:21.720><c> happens</c><00:24:21.900><c> is</c><00:24:22.200><c> we</c><00:24:22.380><c> over</c><00:24:22.559><c> regulate</c><00:24:23.100><c> uh</c><00:24:23.640><c> if</c> what happens is we over regulate uh if what happens is we over regulate uh if you<00:24:23.880><c> look</c><00:24:24.000><c> at</c><00:24:24.120><c> like</c><00:24:24.299><c> what's</c><00:24:24.539><c> happening</c><00:24:24.840><c> with</c> you look at like what's happening with you look at like what's happening with the<00:24:25.200><c> banks</c><00:24:25.559><c> and</c><00:24:26.340><c> all</c><00:24:26.760><c> of</c><00:24:26.940><c> that</c><00:24:27.120><c> and</c><00:24:28.020><c> the</c> the banks and all of that and the the banks and all of that and the response<00:24:28.679><c> is</c><00:24:29.039><c> the</c><00:24:29.460><c> wild</c><00:24:29.580><c> west</c><00:24:29.820><c> of</c><00:24:30.059><c> crypto</c> response is the wild west of crypto response is the wild west of crypto which<00:24:30.659><c> is</c><00:24:30.780><c> the</c><00:24:31.080><c> financial</c><00:24:31.200><c> system</c><00:24:31.559><c> with</c><00:24:31.980><c> no</c> which is the financial system with no which is the financial system with no regulation<00:24:32.760><c> and</c><00:24:33.659><c> I</c><00:24:33.960><c> don't</c><00:24:34.020><c> think</c><00:24:34.140><c> that's</c><00:24:34.320><c> the</c> regulation and I don't think that's the regulation and I don't think that's the right<00:24:34.620><c> answer</c><00:24:34.860><c> either</c><00:24:35.100><c> so</c><00:24:35.940><c> the</c><00:24:36.179><c> answer</c><00:24:36.360><c> is</c> right answer either so the answer is right answer either so the answer is somewhere<00:24:36.900><c> in</c><00:24:37.020><c> the</c><00:24:37.140><c> middle</c> somewhere in the middle somewhere in the middle but<00:24:38.880><c> I</c><00:24:39.000><c> don't</c><00:24:39.059><c> know</c><00:24:39.179><c> where</c><00:24:39.299><c> that</c><00:24:39.480><c> is</c> but I don't know where that is but I don't know where that is um and<00:24:40.500><c> maybe</c><00:24:40.679><c> there's</c><00:24:40.860><c> an</c><00:24:41.039><c> AI</c><00:24:41.220><c> algorithm</c> um and maybe there's an AI algorithm um and maybe there's an AI algorithm that<00:24:41.760><c> can</c><00:24:41.880><c> help</c><00:24:42.000><c> me</c><00:24:42.059><c> find</c><00:24:42.299><c> exactly</c><00:24:42.720><c> where</c><00:24:42.960><c> that</c> that can help me find exactly where that that can help me find exactly where that middle<00:24:43.260><c> is</c><00:24:43.559><c> because</c> middle is because middle is because um<00:24:45.240><c> yeah</c><00:24:45.720><c> I</c><00:24:45.960><c> don't</c><00:24:46.080><c> think</c><00:24:46.260><c> we're</c><00:24:46.799><c> in</c><00:24:46.980><c> a</c><00:24:47.159><c> place</c> um yeah I don't think we're in a place um yeah I don't think we're in a place that<00:24:47.700><c> we</c><00:24:47.880><c> can</c><00:24:48.380><c> successfully</c><00:24:49.380><c> regulate</c><00:24:50.039><c> AI</c><00:24:50.820><c> in</c> that we can successfully regulate AI in that we can successfully regulate AI in any<00:24:51.360><c> sort</c><00:24:51.539><c> of</c><00:24:51.600><c> meaningful</c><00:24:51.900><c> way</c> any sort of meaningful way any sort of meaningful way it<00:24:53.580><c> can't</c><00:24:53.760><c> take</c><00:24:53.940><c> 10</c><00:24:54.120><c> years</c><00:24:54.360><c> from</c><00:24:55.140><c> the</c><00:24:55.320><c> point</c><00:24:55.440><c> at</c> it can't take 10 years from the point at it can't take 10 years from the point at which<00:24:55.740><c> it's</c><00:24:55.860><c> dangerous</c> which it's dangerous which it's dangerous it's<00:24:57.120><c> too</c><00:24:57.419><c> late</c> I<00:25:00.659><c> I'm</c><00:25:00.900><c> not</c><00:25:01.140><c> normally</c><00:25:01.440><c> an</c><00:25:01.679><c> advocate</c><00:25:02.100><c> of</c> I I'm not normally an advocate of I I'm not normally an advocate of Regulation<00:25:03.600><c> and</c><00:25:03.840><c> oversight</c><00:25:04.320><c> I</c><00:25:04.919><c> mean</c><00:25:05.100><c> I</c><00:25:05.220><c> think</c> Regulation and oversight I mean I think Regulation and oversight I mean I think once<00:25:05.820><c> you're</c><00:25:05.940><c> generally</c><00:25:06.179><c> grow</c><00:25:06.539><c> inside</c><00:25:06.720><c> of</c> once you're generally grow inside of once you're generally grow inside of minimizing<00:25:07.380><c> those</c><00:25:07.620><c> things</c><00:25:07.799><c> but</c><00:25:08.700><c> this</c><00:25:09.059><c> is</c><00:25:09.179><c> a</c> minimizing those things but this is a minimizing those things but this is a case<00:25:09.480><c> where</c><00:25:09.780><c> you</c><00:25:09.960><c> have</c><00:25:10.200><c> a</c><00:25:10.799><c> very</c><00:25:10.919><c> serious</c> case where you have a very serious case where you have a very serious danger<00:25:11.460><c> to</c><00:25:11.700><c> the</c><00:25:11.820><c> public</c> danger to the public danger to the public instead<00:25:14.039><c> for</c><00:25:14.159><c> there</c><00:25:14.280><c> needs</c><00:25:14.580><c> to</c><00:25:14.580><c> be</c><00:25:14.700><c> a</c><00:25:14.940><c> public</c> instead for there needs to be a public instead for there needs to be a public body<00:25:15.559><c> that</c><00:25:17.059><c> has</c><00:25:18.059><c> insight</c><00:25:18.539><c> and</c><00:25:18.900><c> then</c><00:25:19.080><c> oversight</c> body that has insight and then oversight body that has insight and then oversight on<00:25:20.039><c> to</c><00:25:20.640><c> confirm</c><00:25:21.000><c> that</c><00:25:21.299><c> everyone</c><00:25:21.480><c> is</c><00:25:22.080><c> again</c> on to confirm that everyone is again on to confirm that everyone is again helping<00:25:23.880><c> AI</c><00:25:24.299><c> safely</c> this<00:25:27.659><c> is</c><00:25:27.779><c> extremely</c><00:25:28.260><c> important</c> this is extremely important this is extremely important I<00:25:30.600><c> think</c><00:25:30.720><c> the</c><00:25:31.020><c> danger</c><00:25:31.260><c> of</c><00:25:31.440><c> AI</c><00:25:31.740><c> is</c><00:25:31.919><c> much</c><00:25:32.340><c> greater</c> I think the danger of AI is much greater I think the danger of AI is much greater than<00:25:32.940><c> the</c><00:25:33.179><c> the</c><00:25:33.360><c> danger</c><00:25:33.900><c> of</c><00:25:34.080><c> nuclear</c><00:25:34.440><c> warheads</c> than the the danger of nuclear warheads than the the danger of nuclear warheads by<00:25:35.039><c> a</c><00:25:35.279><c> lot</c> by a lot by a lot um um um that<00:25:39.059><c> we</c><00:25:39.179><c> allow</c><00:25:39.779><c> anyone</c><00:25:40.620><c> to</c><00:25:41.039><c> just</c><00:25:41.340><c> build</c> that we allow anyone to just build that we allow anyone to just build nuclear<00:25:42.000><c> warheads</c><00:25:42.419><c> if</c><00:25:42.480><c> they</c><00:25:42.600><c> want</c> nuclear warheads if they want nuclear warheads if they want that<00:25:44.159><c> would</c><00:25:44.520><c> be</c><00:25:44.640><c> insane</c><00:25:44.940><c> so</c><00:25:45.600><c> why</c><00:25:45.779><c> do</c><00:25:45.900><c> we</c><00:25:46.080><c> have</c> that would be insane so why do we have that would be insane so why do we have no<00:25:46.320><c> regulatory</c><00:25:46.740><c> oversight</c><00:25:47.159><c> this</c><00:25:48.000><c> is</c><00:25:48.120><c> insane</c> no regulatory oversight this is insane no regulatory oversight this is insane and<00:25:50.760><c> the</c><00:25:50.880><c> intent</c><00:25:51.120><c> with</c><00:25:51.240><c> open</c><00:25:51.419><c> AI</c><00:25:51.840><c> is</c><00:25:52.760><c> to</c> and the intent with open AI is to and the intent with open AI is to democratize<00:25:54.740><c> AI</c><00:25:55.740><c> power</c> democratize AI power democratize AI power there's<00:25:57.720><c> a</c><00:25:57.900><c> quote</c><00:25:58.080><c> that</c><00:25:58.200><c> I</c><00:25:58.320><c> love</c><00:25:58.440><c> from</c><00:25:59.400><c> old</c> there's a quote that I love from old there's a quote that I love from old Acton<00:26:00.659><c> he</c><00:26:00.900><c> was</c><00:26:00.960><c> the</c><00:26:01.140><c> guy</c><00:26:01.260><c> that</c><00:26:01.380><c> came</c><00:26:01.559><c> up</c><00:26:01.620><c> with</c> Acton he was the guy that came up with Acton he was the guy that came up with power<00:26:01.980><c> corrupts</c><00:26:02.580><c> and</c><00:26:02.760><c> absolutely</c><00:26:03.179><c> power</c> power corrupts and absolutely power power corrupts and absolutely power corrupts<00:26:03.779><c> absolutely</c><00:26:04.400><c> which</c><00:26:05.400><c> is</c><00:26:05.520><c> that</c> corrupts absolutely which is that corrupts absolutely which is that freedom<00:26:07.020><c> consists</c><00:26:07.440><c> of</c><00:26:07.559><c> the</c><00:26:07.679><c> distribution</c><00:26:08.039><c> of</c> freedom consists of the distribution of freedom consists of the distribution of power<00:26:08.400><c> and</c><00:26:08.640><c> despotism</c><00:26:09.240><c> in</c><00:26:09.419><c> its</c><00:26:09.659><c> concentration</c> power and despotism in its concentration power and despotism in its concentration and<00:26:11.220><c> so</c><00:26:11.460><c> I</c><00:26:11.640><c> think</c><00:26:11.760><c> it's</c><00:26:12.120><c> important</c><00:26:12.360><c> if</c><00:26:12.659><c> we</c><00:26:12.779><c> have</c> and so I think it's important if we have and so I think it's important if we have this<00:26:13.260><c> incredible</c><00:26:13.620><c> power</c><00:26:13.860><c> of</c><00:26:14.039><c> AI</c><00:26:14.340><c> that</c><00:26:14.640><c> if</c><00:26:14.820><c> not</c> this incredible power of AI that if not this incredible power of AI that if not be<00:26:15.120><c> concentrated</c><00:26:15.600><c> in</c><00:26:15.779><c> the</c><00:26:15.960><c> hands</c><00:26:16.140><c> of</c><00:26:16.200><c> a</c><00:26:16.320><c> few</c> be concentrated in the hands of a few be concentrated in the hands of a few and<00:26:16.740><c> potentially</c><00:26:17.100><c> lead</c><00:26:17.340><c> to</c><00:26:17.580><c> a</c><00:26:18.299><c> world</c><00:26:18.539><c> that</c><00:26:18.840><c> we</c> and potentially lead to a world that we and potentially lead to a world that we don't<00:26:19.500><c> want</c> don't want don't want I'm<00:26:21.299><c> not</c><00:26:21.900><c> okay</c><00:26:22.500><c> let's</c><00:26:23.220><c> touch</c><00:26:23.460><c> on</c><00:26:23.700><c> that</c><00:26:23.820><c> one</c> I'm not okay let's touch on that one I'm not okay let's touch on that one shall<00:26:24.179><c> we</c> shall we shall we um um um so<00:26:26.460><c> I</c><00:26:27.000><c> think</c><00:26:27.120><c> what</c><00:26:27.360><c> we're</c><00:26:27.539><c> actually</c><00:26:27.720><c> finding</c> so I think what we're actually finding so I think what we're actually finding is<00:26:28.500><c> that</c><00:26:28.799><c> AI</c><00:26:29.279><c> is</c><00:26:29.760><c> in</c><00:26:29.940><c> fact</c><00:26:30.059><c> being</c><00:26:30.419><c> democratized</c> is that AI is in fact being democratized is that AI is in fact being democratized because<00:26:31.380><c> we</c><00:26:31.559><c> have</c><00:26:31.679><c> access</c><00:26:31.860><c> to</c><00:26:32.100><c> all</c><00:26:32.279><c> these</c> because we have access to all these because we have access to all these different<00:26:32.700><c> AI</c><00:26:33.120><c> models</c><00:26:33.740><c> uh</c><00:26:34.740><c> through</c><00:26:35.400><c> you</c><00:26:35.820><c> know</c> different AI models uh through you know different AI models uh through you know API<00:26:36.299><c> access</c><00:26:36.600><c> and</c><00:26:37.140><c> all</c><00:26:37.260><c> of</c><00:26:37.380><c> that</c><00:26:37.559><c> and</c><00:26:37.860><c> and</c><00:26:37.980><c> we've</c> API access and all of that and and we've API access and all of that and and we've got<00:26:38.340><c> open</c><00:26:38.520><c> source</c><00:26:38.880><c> models</c><00:26:39.240><c> that</c><00:26:39.360><c> we</c><00:26:39.480><c> can</c> got open source models that we can got open source models that we can actually actually actually deploy<00:26:41.400><c> in</c><00:26:41.520><c> any</c><00:26:41.760><c> number</c><00:26:41.940><c> of</c><00:26:42.059><c> different</c><00:26:42.179><c> ways</c> deploy in any number of different ways deploy in any number of different ways uh<00:26:43.860><c> and</c><00:26:44.039><c> we're</c><00:26:44.400><c> not</c><00:26:44.580><c> locked</c><00:26:44.880><c> out</c><00:26:45.000><c> of</c><00:26:45.120><c> that</c><00:26:45.240><c> now</c> uh and we're not locked out of that now uh and we're not locked out of that now when<00:26:47.520><c> we</c><00:26:47.700><c> look</c><00:26:47.880><c> at</c><00:26:48.240><c> the</c><00:26:48.539><c> recent</c><00:26:48.779><c> past</c> when we look at the recent past when we look at the recent past AI<00:26:51.480><c> power</c><00:26:51.720><c> was</c><00:26:52.520><c> centralized</c><00:26:53.520><c> in</c><00:26:53.760><c> the</c><00:26:53.820><c> hands</c><00:26:54.000><c> of</c> AI power was centralized in the hands of AI power was centralized in the hands of a<00:26:54.179><c> few</c><00:26:54.299><c> it</c><00:26:54.419><c> was</c><00:26:54.539><c> centralized</c><00:26:54.960><c> in</c><00:26:55.380><c> the</c><00:26:55.679><c> social</c> a few it was centralized in the social a few it was centralized in the social media<00:26:56.640><c> the</c><00:26:56.940><c> the</c><00:26:57.120><c> big</c><00:26:57.360><c> Tech</c><00:26:57.539><c> platforms</c><00:26:58.140><c> of</c><00:26:58.260><c> the</c> media the the big Tech platforms of the media the the big Tech platforms of the world world world um<00:26:59.760><c> in</c><00:27:00.360><c> terms</c><00:27:00.659><c> of</c><00:27:00.960><c> you</c><00:27:01.140><c> know</c><00:27:01.200><c> distribution</c> um in terms of you know distribution um in terms of you know distribution algorithms<00:27:02.340><c> in</c><00:27:02.640><c> terms</c><00:27:02.880><c> of</c><00:27:02.940><c> search</c><00:27:03.179><c> rankings</c> algorithms in terms of search rankings algorithms in terms of search rankings like<00:27:03.779><c> you</c><00:27:03.960><c> think</c><00:27:04.080><c> of</c><00:27:04.200><c> the</c><00:27:04.320><c> power</c><00:27:04.440><c> of</c><00:27:04.679><c> Google</c> like you think of the power of Google like you think of the power of Google um<00:27:06.000><c> great</c><00:27:06.240><c> it's</c><00:27:06.419><c> a</c><00:27:06.600><c> search</c><00:27:06.720><c> box</c><00:27:06.960><c> right</c><00:27:07.140><c> what</c> um great it's a search box right what um great it's a search box right what does<00:27:07.559><c> that</c><00:27:07.679><c> actually</c><00:27:07.799><c> have</c><00:27:08.039><c> what</c><00:27:08.340><c> power</c><00:27:08.520><c> does</c> does that actually have what power does does that actually have what power does that<00:27:08.880><c> have</c><00:27:09.059><c> well</c><00:27:09.900><c> you</c><00:27:10.320><c> have</c><00:27:10.440><c> to</c><00:27:10.500><c> realize</c><00:27:10.799><c> that</c> that have well you have to realize that that have well you have to realize that a<00:27:11.340><c> lot</c><00:27:11.400><c> of</c><00:27:11.520><c> businesses</c><00:27:12.059><c> base</c><00:27:12.720><c> their</c><00:27:13.020><c> entire</c> a lot of businesses base their entire a lot of businesses base their entire value<00:27:13.740><c> on</c><00:27:14.220><c> their</c><00:27:14.460><c> search</c><00:27:14.580><c> rankings</c> value on their search rankings value on their search rankings if<00:27:16.260><c> Google's</c><00:27:16.679><c> algorithm</c><00:27:17.039><c> suddenly</c><00:27:17.460><c> decides</c> if Google's algorithm suddenly decides if Google's algorithm suddenly decides that<00:27:18.120><c> it</c><00:27:18.240><c> doesn't</c><00:27:18.360><c> like</c><00:27:18.539><c> your</c><00:27:18.659><c> business</c> that it doesn't like your business that it doesn't like your business anymore<00:27:19.220><c> your</c><00:27:20.220><c> business</c><00:27:20.340><c> could</c><00:27:20.700><c> fail</c><00:27:20.880><c> you're</c> anymore your business could fail you're anymore your business could fail you're talking<00:27:21.720><c> about</c><00:27:21.960><c> control</c><00:27:22.380><c> over</c><00:27:22.980><c> the</c><00:27:23.880><c> outcomes</c> talking about control over the outcomes talking about control over the outcomes you<00:27:24.720><c> know</c><00:27:24.779><c> governing</c><00:27:25.200><c> people's</c><00:27:25.620><c> lives</c><00:27:26.100><c> and</c> you know governing people's lives and you know governing people's lives and there's<00:27:26.520><c> this</c><00:27:26.760><c> constant</c><00:27:27.120><c> back</c><00:27:27.299><c> and</c><00:27:27.419><c> forth</c><00:27:27.659><c> and</c> there's this constant back and forth and there's this constant back and forth and adaptation<00:27:28.440><c> to</c><00:27:28.679><c> it</c> adaptation to it adaptation to it but<00:27:29.820><c> it's</c><00:27:30.059><c> not</c><00:27:30.440><c> adapting</c><00:27:31.440><c> to</c><00:27:31.679><c> the</c><00:27:31.860><c> underlying</c> but it's not adapting to the underlying but it's not adapting to the underlying models<00:27:32.700><c> adapting</c><00:27:33.120><c> to</c><00:27:33.299><c> the</c><00:27:33.419><c> observation</c><00:27:34.020><c> of</c> models adapting to the observation of models adapting to the observation of the<00:27:34.740><c> results</c><00:27:34.919><c> of</c><00:27:35.220><c> interacting</c><00:27:35.760><c> with</c><00:27:36.000><c> that</c> the results of interacting with that the results of interacting with that model<00:27:36.360><c> and</c><00:27:37.260><c> so</c><00:27:37.380><c> I</c><00:27:37.500><c> think</c><00:27:37.620><c> we</c><00:27:37.799><c> can</c><00:27:37.919><c> actually</c> model and so I think we can actually model and so I think we can actually create<00:27:38.460><c> much</c><00:27:39.000><c> more</c><00:27:39.179><c> powerful</c><00:27:39.600><c> ways</c><00:27:40.080><c> of</c> create much more powerful ways of create much more powerful ways of interacting<00:27:41.159><c> with</c> interacting with interacting with algorithms<00:27:43.620><c> that</c><00:27:43.799><c> AI</c><00:27:44.159><c> is</c><00:27:44.400><c> kind</c><00:27:44.700><c> of</c><00:27:44.820><c> with</c><00:27:45.240><c> AI</c><00:27:45.720><c> on</c> algorithms that AI is kind of with AI on algorithms that AI is kind of with AI on our<00:27:46.080><c> side</c><00:27:46.320><c> as</c><00:27:46.620><c> opposed</c><00:27:46.980><c> to</c> our side as opposed to our side as opposed to you<00:27:48.299><c> know</c><00:27:48.360><c> being</c><00:27:48.779><c> subject</c><00:27:49.679><c> to</c><00:27:50.159><c> the</c><00:27:50.460><c> whims</c><00:27:50.760><c> of</c> you know being subject to the whims of you know being subject to the whims of whatever<00:27:51.419><c> platform</c><00:27:51.960><c> we</c><00:27:52.140><c> happen</c><00:27:52.380><c> to</c><00:27:52.500><c> be</c><00:27:52.620><c> using</c> whatever platform we happen to be using whatever platform we happen to be using that<00:27:53.940><c> is</c><00:27:54.120><c> tied</c><00:27:54.360><c> into</c><00:27:54.600><c> our</c><00:27:55.140><c> livelihood</c><00:27:55.679><c> so</c><00:27:56.400><c> I</c> that is tied into our livelihood so I that is tied into our livelihood so I think<00:27:56.700><c> in</c><00:27:56.880><c> this</c><00:27:57.059><c> case</c> think in this case think in this case um<00:27:58.559><c> even</c><00:27:58.799><c> though</c><00:27:58.980><c> it</c><00:27:59.159><c> seems</c><00:27:59.460><c> like</c><00:27:59.640><c> you</c><00:27:59.940><c> know</c> um even though it seems like you know um even though it seems like you know these<00:28:00.240><c> big</c><00:28:00.360><c> companies</c><00:28:00.659><c> are</c><00:28:00.840><c> developing</c><00:28:01.200><c> AI</c> these big companies are developing AI these big companies are developing AI platforms<00:28:02.100><c> which</c><00:28:02.279><c> they</c><00:28:02.580><c> are</c> platforms which they are platforms which they are um<00:28:03.779><c> there's</c><00:28:04.260><c> enough</c><00:28:04.620><c> competition</c><00:28:05.460><c> and</c><00:28:05.880><c> the</c> um there's enough competition and the um there's enough competition and the access<00:28:06.240><c> is</c><00:28:06.600><c> becoming</c><00:28:07.020><c> at</c><00:28:07.260><c> a</c><00:28:07.320><c> low</c><00:28:07.440><c> enough</c><00:28:07.620><c> level</c> access is becoming at a low enough level access is becoming at a low enough level that<00:28:08.700><c> it's</c><00:28:08.880><c> democratizing</c><00:28:09.600><c> the</c><00:28:09.720><c> access</c><00:28:09.900><c> as</c> that it's democratizing the access as that it's democratizing the access as opposed<00:28:10.740><c> to</c><00:28:10.799><c> concentrating</c><00:28:11.460><c> again</c><00:28:11.640><c> in</c><00:28:11.880><c> the</c> opposed to concentrating again in the opposed to concentrating again in the hands<00:28:12.240><c> of</c><00:28:12.480><c> the</c><00:28:12.659><c> large</c><00:28:12.779><c> tech</c><00:28:13.080><c> companies</c><00:28:13.500><c> which</c> hands of the large tech companies which hands of the large tech companies which is<00:28:14.220><c> what</c><00:28:14.400><c> we've</c><00:28:14.580><c> seen</c><00:28:14.820><c> for</c><00:28:15.120><c> the</c><00:28:15.240><c> past</c><00:28:15.480><c> 20</c><00:28:16.020><c> plus</c> is what we've seen for the past 20 plus is what we've seen for the past 20 plus years<00:28:16.580><c> really</c><00:28:17.580><c> all</c><00:28:17.760><c> that</c><00:28:17.940><c> worried</c><00:28:18.179><c> about</c><00:28:18.299><c> the</c> years really all that worried about the years really all that worried about the short-term<00:28:19.020><c> stuff</c><00:28:19.260><c> the</c><00:28:19.980><c> things</c><00:28:20.279><c> that</c><00:28:20.520><c> are</c> short-term stuff the things that are short-term stuff the things that are like<00:28:22.500><c> narrow</c><00:28:22.919><c> AI</c><00:28:23.279><c> is</c><00:28:23.580><c> not</c><00:28:23.760><c> a</c><00:28:23.940><c> species</c><00:28:24.240><c> level</c> like narrow AI is not a species level like narrow AI is not a species level risk risk risk it<00:28:27.000><c> will</c><00:28:27.840><c> result</c><00:28:28.020><c> in</c><00:28:28.320><c> dislocation</c><00:28:28.919><c> uh</c><00:28:29.760><c> in</c><00:28:30.179><c> lost</c> it will result in dislocation uh in lost it will result in dislocation uh in lost jobs<00:28:30.900><c> and</c><00:28:32.059><c> do</c><00:28:33.059><c> you</c><00:28:33.179><c> think</c><00:28:33.240><c> we</c><00:28:33.299><c> need</c><00:28:33.419><c> to</c><00:28:33.539><c> account</c> jobs and do you think we need to account jobs and do you think we need to account for<00:28:33.900><c> that</c><00:28:34.020><c> you</c><00:28:34.320><c> know</c><00:28:34.380><c> that</c><00:28:34.620><c> sort</c><00:28:35.460><c> of</c><00:28:35.580><c> better</c> for that you know that sort of better for that you know that sort of better weaponry<00:28:36.240><c> and</c><00:28:36.419><c> that</c><00:28:36.600><c> kind</c><00:28:36.720><c> of</c><00:28:36.779><c> thing</c><00:28:36.900><c> but</c><00:28:37.440><c> it</c> weaponry and that kind of thing but it weaponry and that kind of thing but it is<00:28:37.740><c> not</c><00:28:37.919><c> a</c><00:28:38.039><c> fundamental</c><00:28:38.580><c> species</c><00:28:39.120><c> level</c><00:28:39.360><c> risk</c> is not a fundamental species level risk is not a fundamental species level risk uh<00:28:40.620><c> whereas</c><00:28:41.039><c> uh</c><00:28:41.820><c> digital</c><00:28:42.059><c> super</c><00:28:42.299><c> intelligence</c> uh whereas uh digital super intelligence uh whereas uh digital super intelligence is is is so<00:28:44.600><c> it's</c><00:28:45.600><c> really</c><00:28:45.779><c> all</c><00:28:46.140><c> about</c><00:28:46.320><c> laying</c><00:28:46.919><c> the</c> so it's really all about laying the so it's really all about laying the groundwork<00:28:47.340><c> to</c><00:28:47.580><c> make</c><00:28:47.760><c> sure</c><00:28:47.940><c> that</c> groundwork to make sure that groundwork to make sure that if<00:28:49.500><c> if</c><00:28:49.620><c> Humanity</c><00:28:50.279><c> collectively</c><00:28:50.880><c> decides</c><00:28:51.419><c> that</c> if if Humanity collectively decides that if if Humanity collectively decides that creating<00:28:53.039><c> digital</c><00:28:53.340><c> super</c><00:28:53.580><c> intelligence</c><00:28:54.059><c> is</c> creating digital super intelligence is creating digital super intelligence is the<00:28:54.480><c> right</c><00:28:54.659><c> move</c> the right move the right move then then then we<00:28:57.659><c> should</c><00:28:57.779><c> do</c><00:28:57.960><c> so</c><00:28:58.140><c> very</c><00:28:58.620><c> very</c><00:28:58.919><c> carefully</c> very<00:29:03.299><c> very</c><00:29:03.480><c> carefully</c><00:29:04.039><c> we</c><00:29:05.039><c> were</c><00:29:05.220><c> rapidly</c> very very carefully we were rapidly very very carefully we were rapidly headed<00:29:05.880><c> towards</c><00:29:06.120><c> digital</c><00:29:06.539><c> super</c> headed towards digital super headed towards digital super intelligence<00:29:07.260><c> that</c><00:29:07.440><c> far</c><00:29:07.620><c> exceeds</c><00:29:08.039><c> any</c><00:29:08.220><c> human</c> intelligence that far exceeds any human intelligence that far exceeds any human Okay<00:29:10.500><c> so</c> Okay so Okay so kind<00:29:13.679><c> of</c><00:29:13.860><c> on</c><00:29:13.919><c> this</c><00:29:14.100><c> last</c><00:29:14.220><c> bit</c><00:29:14.400><c> here</c> kind of on this last bit here kind of on this last bit here um<00:29:15.480><c> you</c><00:29:16.200><c> know</c><00:29:16.320><c> I</c><00:29:16.559><c> think</c><00:29:16.679><c> we</c><00:29:17.220><c> do</c> um you know I think we do um you know I think we do uh uh uh we<00:29:20.460><c> do</c><00:29:20.700><c> need</c><00:29:20.880><c> to</c><00:29:21.000><c> think</c><00:29:21.179><c> about</c><00:29:21.600><c> the</c><00:29:22.080><c> potential</c> we do need to think about the potential we do need to think about the potential consequences<00:29:22.860><c> of</c><00:29:23.100><c> this</c><00:29:23.279><c> but</c><00:29:24.000><c> I</c><00:29:24.779><c> also</c><00:29:25.020><c> don't</c> consequences of this but I also don't consequences of this but I also don't think<00:29:25.559><c> that</c> think that think that the<00:29:27.299><c> artificial</c><00:29:27.659><c> super</c><00:29:27.960><c> intelligence</c><00:29:28.620><c> will</c> the artificial super intelligence will the artificial super intelligence will arise<00:29:29.340><c> in</c><00:29:29.640><c> a</c><00:29:29.820><c> way</c> arise in a way arise in a way that<00:29:31.140><c> most</c><00:29:31.380><c> people</c><00:29:31.559><c> imagine</c><00:29:31.799><c> I</c><00:29:32.340><c> don't</c><00:29:32.460><c> think</c> that most people imagine I don't think that most people imagine I don't think it's<00:29:32.700><c> going</c><00:29:32.940><c> to</c><00:29:33.059><c> be</c><00:29:33.240><c> a</c><00:29:33.720><c> single</c><00:29:34.020><c> super</c> it's going to be a single super it's going to be a single super intelligence<00:29:34.919><c> I</c><00:29:35.220><c> think</c><00:29:35.580><c> we</c><00:29:35.880><c> already</c><00:29:36.059><c> have</c> intelligence I think we already have intelligence I think we already have um<00:29:38.640><c> reached</c><00:29:39.539><c> a</c><00:29:39.720><c> point</c><00:29:40.080><c> where</c><00:29:40.860><c> there's</c><00:29:41.340><c> a</c><00:29:41.700><c> lot</c> um reached a point where there's a lot um reached a point where there's a lot happening<00:29:42.380><c> uh</c><00:29:43.380><c> in</c><00:29:43.500><c> these</c><00:29:43.860><c> AI</c><00:29:44.159><c> models</c><00:29:44.580><c> that</c><00:29:44.760><c> we</c> happening uh in these AI models that we happening uh in these AI models that we don't<00:29:45.059><c> necessarily</c><00:29:45.539><c> fully</c><00:29:45.960><c> understand</c><00:29:46.140><c> we</c> don't necessarily fully understand we don't necessarily fully understand we have<00:29:47.039><c> a</c><00:29:47.159><c> lot</c><00:29:47.220><c> of</c><00:29:47.279><c> emergent</c><00:29:47.640><c> properties</c><00:29:48.000><c> that</c> have a lot of emergent properties that have a lot of emergent properties that we're<00:29:48.419><c> seeing</c><00:29:48.720><c> we're</c><00:29:49.320><c> trying</c><00:29:49.500><c> to</c><00:29:49.679><c> figure</c> we're seeing we're trying to figure we're seeing we're trying to figure those<00:29:50.100><c> out</c> those out those out um<00:29:52.080><c> but</c><00:29:52.620><c> I</c><00:29:52.740><c> don't</c><00:29:52.860><c> think</c><00:29:52.980><c> there's</c><00:29:53.159><c> actually</c><00:29:53.399><c> a</c> um but I don't think there's actually a um but I don't think there's actually a great<00:29:53.700><c> understanding</c><00:29:54.240><c> of</c><00:29:54.419><c> what</c><00:29:54.600><c> intelligence</c> great understanding of what intelligence great understanding of what intelligence is is is um<00:29:56.700><c> and</c><00:29:57.360><c> so</c> um and so um and so a<00:30:00.659><c> general</c><00:30:01.140><c> kind</c><00:30:01.799><c> of</c><00:30:01.980><c> super</c><00:30:02.399><c> intelligence</c> a general kind of super intelligence a general kind of super intelligence um um um I<00:30:06.659><c> don't</c><00:30:06.779><c> know</c><00:30:06.960><c> that</c><00:30:07.140><c> we'll</c><00:30:07.320><c> know</c><00:30:07.500><c> it</c><00:30:07.679><c> when</c><00:30:07.860><c> we</c> I don't know that we'll know it when we I don't know that we'll know it when we see<00:30:08.159><c> it</c><00:30:08.640><c> uh</c> see it uh see it uh and<00:30:11.399><c> so</c><00:30:11.640><c> that's</c><00:30:11.820><c> kind</c><00:30:12.000><c> of</c><00:30:12.120><c> an</c><00:30:12.240><c> interesting</c> and so that's kind of an interesting and so that's kind of an interesting piece<00:30:13.080><c> of</c><00:30:13.200><c> this</c><00:30:13.320><c> puzzle</c> piece of this puzzle piece of this puzzle um<00:30:14.940><c> I</c><00:30:15.240><c> don't</c><00:30:15.539><c> know</c><00:30:15.659><c> that</c><00:30:15.779><c> anyone</c><00:30:16.140><c> can</c><00:30:16.500><c> really</c> um I don't know that anyone can really um I don't know that anyone can really Define<00:30:17.419><c> what</c><00:30:18.419><c> uh</c><00:30:19.200><c> the</c><00:30:19.679><c> this</c> Define what uh the this Define what uh the this super<00:30:21.120><c> intelligence</c><00:30:21.659><c> will</c><00:30:22.020><c> look</c><00:30:22.440><c> like</c><00:30:22.620><c> or</c> super intelligence will look like or super intelligence will look like or what<00:30:23.039><c> it</c><00:30:23.159><c> will</c><00:30:23.340><c> do</c><00:30:23.520><c> or</c><00:30:23.820><c> how</c><00:30:23.940><c> we'll</c><00:30:24.120><c> know</c><00:30:24.360><c> that</c> what it will do or how we'll know that what it will do or how we'll know that it<00:30:24.720><c> exists</c> it exists it exists um but<00:30:29.760><c> what</c><00:30:30.659><c> I</c><00:30:30.779><c> like</c><00:30:30.960><c> to</c><00:30:31.200><c> view</c><00:30:31.440><c> as</c><00:30:32.340><c> the</c><00:30:32.760><c> digital</c> but what I like to view as the digital but what I like to view as the digital super<00:30:33.240><c> intelligence</c><00:30:33.840><c> kind</c><00:30:34.740><c> of</c><00:30:34.860><c> the</c><00:30:34.980><c> the</c> super intelligence kind of the the super intelligence kind of the the singular<00:30:35.700><c> digital</c><00:30:36.120><c> super</c><00:30:36.360><c> intelligence</c><00:30:37.159><c> is</c> singular digital super intelligence is singular digital super intelligence is actually<00:30:38.580><c> the</c> actually the actually the collection<00:30:42.360><c> of</c><00:30:42.840><c> humans</c><00:30:43.260><c> as</c><00:30:43.620><c> a</c><00:30:43.799><c> species</c><00:30:44.659><c> using</c> collection of humans as a species using collection of humans as a species using high-tech high-tech high-tech uh<00:30:48.419><c> you</c><00:30:48.840><c> know</c><00:30:48.960><c> models</c><00:30:49.440><c> algorithms</c><00:30:50.340><c> Etc</c><00:30:50.539><c> to</c><00:30:51.539><c> be</c> uh you know models algorithms Etc to be uh you know models algorithms Etc to be able<00:30:51.779><c> to</c><00:30:51.960><c> solve</c><00:30:52.200><c> some</c><00:30:52.320><c> of</c><00:30:52.500><c> Humanity's</c> able to solve some of Humanity's able to solve some of Humanity's greatest<00:30:53.100><c> problems</c><00:30:53.760><c> and</c><00:30:54.179><c> the</c><00:30:54.419><c> super</c> greatest problems and the super greatest problems and the super intelligence<00:30:55.020><c> arises</c><00:30:55.740><c> as</c><00:30:56.480><c> a</c><00:30:57.480><c> collective</c> intelligence arises as a collective intelligence arises as a collective application<00:30:59.360><c> of</c><00:31:00.360><c> technology</c><00:31:01.080><c> to</c><00:31:01.679><c> our</c> application of technology to our application of technology to our greatest<00:31:02.039><c> problems</c><00:31:03.080><c> and</c><00:31:04.080><c> that</c><00:31:04.740><c> becomes</c><00:31:05.399><c> the</c> greatest problems and that becomes the greatest problems and that becomes the digital<00:31:06.360><c> super</c><00:31:06.600><c> intelligence</c><00:31:07.200><c> that</c><00:31:07.559><c> elevates</c> digital super intelligence that elevates digital super intelligence that elevates Humanity<00:31:08.399><c> to</c><00:31:08.580><c> the</c><00:31:08.640><c> next</c><00:31:08.760><c> level</c><00:31:08.940><c> I</c><00:31:09.720><c> don't</c><00:31:09.840><c> think</c> Humanity to the next level I don't think Humanity to the next level I don't think it's<00:31:10.080><c> going</c><00:31:10.260><c> to</c><00:31:10.380><c> be</c><00:31:10.500><c> a</c><00:31:10.799><c> singular</c><00:31:11.159><c> entity</c><00:31:12.120><c> with</c> it's going to be a singular entity with it's going to be a singular entity with its<00:31:13.020><c> own</c><00:31:13.080><c> desires</c><00:31:13.559><c> I</c><00:31:13.799><c> think</c><00:31:13.980><c> it's</c><00:31:14.220><c> going</c><00:31:14.460><c> to</c><00:31:14.700><c> be</c> its own desires I think it's going to be its own desires I think it's going to be this<00:31:15.179><c> use</c><00:31:15.419><c> of</c><00:31:15.659><c> technology</c><00:31:16.140><c> to</c><00:31:17.000><c> overcome</c><00:31:18.000><c> our</c> this use of technology to overcome our this use of technology to overcome our limitations<00:31:18.740><c> to</c><00:31:19.740><c> become</c><00:31:20.039><c> a</c><00:31:20.220><c> multi-planetary</c> limitations to become a multi-planetary limitations to become a multi-planetary species species species and<00:31:22.679><c> to</c><00:31:22.919><c> truly</c><00:31:23.220><c> achieve</c><00:31:23.840><c> uh</c><00:31:24.840><c> you</c><00:31:25.020><c> know</c><00:31:25.080><c> what</c> and to truly achieve uh you know what and to truly achieve uh you know what can<00:31:25.679><c> be</c><00:31:25.799><c> the</c><00:31:25.919><c> golden</c><00:31:26.100><c> age</c><00:31:26.340><c> I</c><00:31:26.580><c> think</c><00:31:26.640><c> we</c><00:31:26.760><c> have</c><00:31:26.880><c> a</c> can be the golden age I think we have a can be the golden age I think we have a long<00:31:27.059><c> way</c><00:31:27.179><c> to</c><00:31:27.360><c> go</c><00:31:27.419><c> to</c><00:31:27.539><c> get</c><00:31:27.720><c> there</c><00:31:27.840><c> but</c><00:31:28.620><c> I</c><00:31:28.919><c> don't</c> long way to go to get there but I don't long way to go to get there but I don't think<00:31:29.220><c> AI</c><00:31:29.760><c> is</c><00:31:30.120><c> going</c><00:31:30.299><c> to</c><00:31:30.419><c> be</c><00:31:30.720><c> the</c> think AI is going to be the think AI is going to be the problem<00:31:33.419><c> I</c><00:31:34.020><c> think</c><00:31:34.140><c> humans</c><00:31:34.500><c> are</c><00:31:34.679><c> uh</c><00:31:35.279><c> it</c><00:31:35.460><c> all</c> problem I think humans are uh it all problem I think humans are uh it all comes<00:31:35.940><c> back</c><00:31:36.059><c> to</c><00:31:36.240><c> humans</c><00:31:36.600><c> I</c><00:31:37.320><c> think</c><00:31:37.440><c> we</c><00:31:38.220><c> are</c><00:31:38.460><c> the</c> comes back to humans I think we are the comes back to humans I think we are the existential<00:31:39.179><c> threat</c><00:31:39.659><c> to</c><00:31:39.960><c> ourselves</c><00:31:40.320><c> much</c> existential threat to ourselves much existential threat to ourselves much more<00:31:41.279><c> so</c><00:31:42.059><c> than</c><00:31:42.419><c> any</c><00:31:42.779><c> sort</c><00:31:43.020><c> of</c><00:31:43.080><c> AI</c><00:31:43.440><c> technology</c> more so than any sort of AI technology more so than any sort of AI technology but<00:31:44.399><c> anyway</c><00:31:45.360><c> that's</c><00:31:46.080><c> all</c><00:31:46.260><c> I've</c><00:31:46.380><c> got</c><00:31:46.559><c> to</c><00:31:46.679><c> say</c> but anyway that's all I've got to say but anyway that's all I've got to say about<00:31:46.860><c> this</c><00:31:47.100><c> for</c><00:31:47.279><c> now</c> about this for now about this for now um<00:31:48.360><c> you</c><00:31:48.899><c> know</c><00:31:48.960><c> I'm</c><00:31:49.140><c> sure</c><00:31:49.260><c> I'll</c><00:31:49.440><c> have</c><00:31:49.679><c> uh</c><00:31:50.520><c> more</c> um you know I'm sure I'll have uh more um you know I'm sure I'll have uh more videos<00:31:51.360><c> to</c><00:31:51.659><c> react</c><00:31:51.899><c> to</c><00:31:52.020><c> uh</c><00:31:52.559><c> if</c><00:31:52.740><c> you</c><00:31:52.799><c> have</c><00:31:52.860><c> any</c> videos to react to uh if you have any videos to react to uh if you have any you<00:31:53.159><c> want</c><00:31:53.220><c> me</c><00:31:53.340><c> to</c><00:31:53.460><c> check</c><00:31:53.640><c> out</c><00:31:53.820><c> drop</c><00:31:54.539><c> them</c><00:31:54.720><c> in</c> you want me to check out drop them in you want me to check out drop them in the<00:31:54.960><c> comments</c> the comments the comments um<00:31:56.580><c> I'd</c><00:31:57.179><c> be</c><00:31:57.299><c> happy</c><00:31:57.419><c> to</c><00:31:57.600><c> react</c><00:31:57.779><c> to</c><00:31:57.899><c> them</c><00:31:58.020><c> and</c><00:31:58.559><c> uh</c> um I'd be happy to react to them and uh um I'd be happy to react to them and uh thanks<00:31:58.980><c> for</c><00:31:59.159><c> watching</c><00:31:59.399><c> I'll</c><00:31:59.700><c> see</c><00:31:59.880><c> the</c><00:32:00.059><c> next</c> thanks for watching I'll see the next thanks for watching I'll see the next video",
  "transcript_source": "youtube-subs"
}